#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Pandoc Wordæ–‡æ¡£å¤„ç†å·¥å…· - å¢å¼ºç‰ˆ (æ”¯æŒåŠ ç‚¹å­—æ£€æµ‹ï¼Œå›¾ç‰‡å¤„ç†å·²ç¦ç”¨)

ä½¿ç”¨pandocå°†Wordæ–‡æ¡£è½¬æ¢ä¸ºæ¨¡å‹å¯è¯»çš„çº¯æ–‡æœ¬å†…å®¹ï¼Œæ”¯æŒï¼š
1. æ–‡æ¡£æ–‡æœ¬è½¬æ¢ (Pandoc)
2. å¤§æ¨¡å‹APIè°ƒç”¨ (æ–‡æ¡£ç»“æ„è§£æ)
3. ç€é‡å·æ£€æµ‹ (åŠ ç‚¹å­—æ ‡è®°ä¿ç•™)
4. è¿ç»­çŸ­æ¨ªçº¿è½¬ä¸­æ–‡ç ´æŠ˜å·

æ³¨ï¼šå›¾ç‰‡æå–å’Œå†…å®¹åˆ†æåŠŸèƒ½å·²ç¦ç”¨ä»¥æé«˜è¿è¡Œæ•ˆç‡
æ³¨ï¼šæ ¼å¼å¢å¼ºåŠŸèƒ½ç”Ÿæˆpandocæ ‡è®°ï¼Œä¾›åç»­æ¨¡å‹å¤„ç†

ä¾èµ–å®‰è£…ï¼š
1. ç¡®ä¿ç³»ç»Ÿå·²å®‰è£…pandoc: https://pandoc.org/installing.html
2. å®‰è£…python-docx: pip install python-docx (ä»…ç”¨äºåŠ ç‚¹å­—é¢„å¤„ç†)
3. å®‰è£…å…¶ä»–ä¾èµ–: pip install requests

ä½¿ç”¨æ–¹æ³•ï¼š
1. è¿è¡Œè„šæœ¬å¤„ç†Wordæ–‡æ¡£
2. æ£€æµ‹å¹¶ä¿ç•™æ‰€æœ‰æ ¼å¼æ ‡è®°ï¼ˆåŸå§‹pandocæ ¼å¼ï¼‰
3. ç”Ÿæˆæœ€ç»ˆçš„è§£æç»“æœ
"""

import subprocess
import requests
import json
import time
import os
import re
import zipfile
import tempfile
from datetime import datetime
from pathlib import Path
from PIL import Image
import base64
from io import BytesIO
from collections import defaultdict

# æ–°å¢ï¼šç‰¹æ®Šæ ¼å¼è¯†åˆ«ä¾èµ–
try:
    from docx import Document
    from docx.enum.text import WD_UNDERLINE
    from docx.oxml.ns import qn
    DOCX_AVAILABLE = True
    print("âœ… python-docxåº“å¯ç”¨ï¼Œæ”¯æŒç‰¹æ®Šæ ¼å¼è¯†åˆ«")
except ImportError:
    DOCX_AVAILABLE = False
    print("âš ï¸ python-docxåº“ä¸å¯ç”¨ï¼Œå°†è·³è¿‡ç‰¹æ®Šæ ¼å¼è¯†åˆ«åŠŸèƒ½")
    print("   å®‰è£…å‘½ä»¤: pip install python-docx")

class PandocWordProcessor:
    def __init__(self):
        self.api_key = "baf9ea42-7e17-4df6-9a22-90127ac8220e"
        self.base_url = "https://ark.cn-beijing.volces.com/api"
        
        # æ£€æŸ¥pandocæ˜¯å¦å¯ç”¨
        self.pandoc_available = self._check_pandoc()
        if not self.pandoc_available:
            print("âš ï¸ è­¦å‘Š: pandocæœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­")
            print("è¯·è®¿é—® https://pandoc.org/installing.html å®‰è£…pandoc")
        
        # æ–°å¢ï¼šç‰¹æ®Šæ ¼å¼è¯†åˆ«åŠŸèƒ½åˆå§‹åŒ–
        self.format_detection_enabled = DOCX_AVAILABLE
        self.special_formatted_text = []
        self.format_statistics = defaultdict(int)
        self.paragraph_formatting = []  # æ–°å¢ï¼šå­˜å‚¨æ®µè½æ ¼å¼ä¿¡æ¯
        
        if self.format_detection_enabled:
            self._init_format_styles()
    
    def _check_pandoc(self):
        """æ£€æŸ¥pandocæ˜¯å¦å¯ç”¨"""
        try:
            result = subprocess.run(['pandoc', '--version'], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                print(f"âœ… Pandocå¯ç”¨: {result.stdout.split()[1]}")
                return True
            else:
                print(f"âŒ Pandocæ£€æŸ¥å¤±è´¥: {result.stderr}")
                return False
        except FileNotFoundError:
            print("âŒ æœªæ‰¾åˆ°pandocå‘½ä»¤")
            return False
        except subprocess.TimeoutExpired:
            print("âŒ Pandocæ£€æŸ¥è¶…æ—¶")
            return False
        except Exception as e:
            print(f"âŒ Pandocæ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    def _init_format_styles(self):
        """åˆå§‹åŒ–æ ¼å¼æ ·å¼æ˜ å°„"""
        if not DOCX_AVAILABLE:
            return
        
        # ä¸‹åˆ’çº¿æ ·å¼æ˜ å°„ - å®‰å…¨åœ°æ·»åŠ ä¸‹åˆ’çº¿æ ·å¼
        self.underline_styles = {}
        styles_to_add = [
            (getattr(WD_UNDERLINE, 'SINGLE', None), "å•ä¸‹åˆ’çº¿"),
            (getattr(WD_UNDERLINE, 'DOUBLE', None), "åŒä¸‹åˆ’çº¿"),
            (getattr(WD_UNDERLINE, 'THICK', None), "ç²—ä¸‹åˆ’çº¿"),
            (getattr(WD_UNDERLINE, 'DOTTED', None), "ç‚¹çŠ¶ä¸‹åˆ’çº¿"),
            (getattr(WD_UNDERLINE, 'DASH', None), "è™šçº¿ä¸‹åˆ’çº¿"),
            (getattr(WD_UNDERLINE, 'DOT_DASH', None), "ç‚¹åˆ’çº¿ä¸‹åˆ’çº¿"),
            (getattr(WD_UNDERLINE, 'DOT_DOT_DASH', None), "ç‚¹ç‚¹åˆ’çº¿ä¸‹åˆ’çº¿"),
            (getattr(WD_UNDERLINE, 'WAVY', None), "æ³¢æµªçº¿ä¸‹åˆ’çº¿"),
            (getattr(WD_UNDERLINE, 'DOTTED_HEAVY', None), "ç²—ç‚¹çŠ¶ä¸‹åˆ’çº¿"),
            (getattr(WD_UNDERLINE, 'DASH_HEAVY', None), "ç²—è™šçº¿ä¸‹åˆ’çº¿"),
            (getattr(WD_UNDERLINE, 'WAVY_HEAVY', None), "ç²—æ³¢æµªçº¿ä¸‹åˆ’çº¿"),
            (getattr(WD_UNDERLINE, 'WAVY_DOUBLE', None), "åŒæ³¢æµªçº¿ä¸‹åˆ’çº¿")
        ]
        
        for style_enum, style_name in styles_to_add:
            if style_enum is not None:
                self.underline_styles[style_enum] = style_name
        
        print(f"ğŸ“‹ åˆå§‹åŒ–äº† {len(self.underline_styles)} ç§ä¸‹åˆ’çº¿æ ·å¼è¯†åˆ«")
    
    def _analyze_text_formatting(self, run, para_index=0, run_index=0):
        """åˆ†ææ–‡æœ¬ç‰‡æ®µçš„æ ¼å¼"""
        if not DOCX_AVAILABLE:
            return []
        
        formats = []
        font = run.font
        
        # ä¸‹åˆ’çº¿æ£€æŸ¥
        if font.underline:
            underline_style = self.underline_styles.get(font.underline, f"æœªçŸ¥ä¸‹åˆ’çº¿æ ·å¼({font.underline})")
            formats.append(f"ä¸‹åˆ’çº¿: {underline_style}")
            
            # ç‰¹åˆ«æ ‡è®°æ³¢æµªçº¿å’Œç‚¹çŠ¶çº¿
            wavy_styles = [style for style in [
                getattr(WD_UNDERLINE, 'WAVY', None),
                getattr(WD_UNDERLINE, 'WAVY_HEAVY', None),
                getattr(WD_UNDERLINE, 'WAVY_DOUBLE', None)
            ] if style is not None]
            
            dotted_styles = [style for style in [
                getattr(WD_UNDERLINE, 'DOTTED', None),
                getattr(WD_UNDERLINE, 'DOTTED_HEAVY', None)
            ] if style is not None]
            
            if font.underline in wavy_styles:
                formats.append("âš ï¸ æ³¢æµªçº¿æ ¼å¼")
            elif font.underline in dotted_styles:
                formats.append("âš ï¸ ç‚¹çŠ¶çº¿æ ¼å¼")
        
        # ä¸Šæ ‡ä¸‹æ ‡
        if font.superscript:
            formats.append("ä¸Šæ ‡")
        if font.subscript:
            formats.append("ä¸‹æ ‡")
        
        # åˆ é™¤çº¿
        if font.strike:
            formats.append("åˆ é™¤çº¿")
        
        # ç²—ä½“æ–œä½“
        if font.bold:
            formats.append("ç²—ä½“")
        if font.italic:
            formats.append("æ–œä½“")
        
        # å­—ä½“é¢œè‰²
        if font.color and font.color.rgb:
            try:
                rgb_val = font.color.rgb
                if hasattr(rgb_val, '__int__'):
                    color_hex = f"#{int(rgb_val):06x}"
                else:
                    color_hex = str(rgb_val)
                formats.append(f"å­—ä½“é¢œè‰²: {color_hex}")
            except Exception:
                formats.append("å­—ä½“é¢œè‰²: ç‰¹æ®Šé¢œè‰²")
        
        # å­—ä½“å¤§å°
        if font.size:
            try:
                size_pt = font.size.pt
                formats.append(f"å­—å·: {size_pt}ç£…")
            except:
                formats.append("å­—å·: è‡ªå®šä¹‰å¤§å°")
        
        # å­—ä½“åç§°
        if font.name:
            formats.append(f"å­—ä½“: {font.name}")
        
        # æ£€æŸ¥ç€é‡å·
        emphasis_mark = self._check_emphasis_mark(run)
        if emphasis_mark:
            formats.append(f"ç€é‡å·: {emphasis_mark}")
        
        # ç»Ÿè®¡æ ¼å¼ä½¿ç”¨
        for fmt in formats:
            self.format_statistics[fmt] += 1
        
        # ä¿å­˜ç‰¹æ®Šæ ¼å¼çš„æ–‡æœ¬
        if formats and run.text.strip():
            self.special_formatted_text.append({
                'text': run.text,
                'paragraph': para_index,
                'run': run_index,
                'formats': formats
            })
        
        return formats
    
    def _check_emphasis_mark(self, run):
        """æ£€æŸ¥ç€é‡å·ï¼ˆåŠ ç‚¹å­—ï¼‰"""
        try:
            run_xml = run._element
            em_elements = run_xml.xpath('.//w:em', namespaces={'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'})
            
            if em_elements:
                em_val = em_elements[0].get(qn('w:val'))
                emphasis_types = {
                    'dot': 'ç‚¹',
                    'comma': 'é€—å·',
                    'circle': 'åœ†åœˆ',
                    'underDot': 'ä¸‹ç‚¹'
                }
                return emphasis_types.get(em_val, f'ç€é‡å·({em_val})')
        except:
            pass
        
        return None
    
    def _analyze_paragraph_formatting(self, paragraph, para_index=0):
        """åˆ†ææ®µè½çš„æ ¼å¼ï¼ŒåŒ…æ‹¬é¦–è¡Œç¼©è¿›"""
        if not DOCX_AVAILABLE:
            return []
        
        para_formats = []
        
        try:
            # è·å–æ®µè½æ ¼å¼
            para_format = paragraph.paragraph_format
            
            # æ£€æŸ¥é¦–è¡Œç¼©è¿›
            if para_format.first_line_indent:
                indent_value = para_format.first_line_indent
                # è½¬æ¢ä¸ºç£…æ•°ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                try:
                    indent_pt = indent_value.pt if hasattr(indent_value, 'pt') else None
                    if indent_pt and indent_pt > 0:
                        para_formats.append(f"é¦–è¡Œç¼©è¿›: {indent_pt:.1f}ç£…")
                        # æ ‡è®°è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„æ ¼å¼ä¿¡æ¯
                        para_formats.append("âš ï¸ é¦–è¡Œç¼©è¿›æ®µè½")
                except:
                    para_formats.append("é¦–è¡Œç¼©è¿›: è‡ªå®šä¹‰å€¼")
                    para_formats.append("âš ï¸ é¦–è¡Œç¼©è¿›æ®µè½")
            
            # æ£€æŸ¥å·¦ç¼©è¿›
            if para_format.left_indent:
                try:
                    left_indent_pt = para_format.left_indent.pt if hasattr(para_format.left_indent, 'pt') else None
                    if left_indent_pt and left_indent_pt > 0:
                        para_formats.append(f"å·¦ç¼©è¿›: {left_indent_pt:.1f}ç£…")
                except:
                    para_formats.append("å·¦ç¼©è¿›: è‡ªå®šä¹‰å€¼")
            
            # æ£€æŸ¥å³ç¼©è¿›
            if para_format.right_indent:
                try:
                    right_indent_pt = para_format.right_indent.pt if hasattr(para_format.right_indent, 'pt') else None
                    if right_indent_pt and right_indent_pt > 0:
                        para_formats.append(f"å³ç¼©è¿›: {right_indent_pt:.1f}ç£…")
                except:
                    para_formats.append("å³ç¼©è¿›: è‡ªå®šä¹‰å€¼")
            
            # æ£€æŸ¥å¯¹é½æ–¹å¼
            if para_format.alignment:
                alignment_names = {
                    0: "å·¦å¯¹é½",
                    1: "å±…ä¸­",
                    2: "å³å¯¹é½",
                    3: "ä¸¤ç«¯å¯¹é½",
                    4: "åˆ†æ•£å¯¹é½"
                }
                alignment_name = alignment_names.get(para_format.alignment, f"å¯¹é½æ–¹å¼{para_format.alignment}")
                para_formats.append(f"å¯¹é½: {alignment_name}")
            
            # ç»Ÿè®¡æ ¼å¼ä½¿ç”¨
            for fmt in para_formats:
                self.format_statistics[fmt] += 1
            
        except Exception as e:
            print(f"  âš ï¸ æ®µè½æ ¼å¼åˆ†æå¤±è´¥: {e}")
        
        return para_formats
    
    def extract_format_analysis(self, docx_path):
        """æå–æ–‡æ¡£çš„æ ¼å¼åˆ†æä¿¡æ¯"""
        if not self.format_detection_enabled:
            print("ğŸš« æ ¼å¼æ£€æµ‹åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡æ ¼å¼åˆ†æ")
            return None
        
        print("ğŸ” å¼€å§‹åˆ†ææ–‡æ¡£æ ¼å¼...")
        
        try:
            doc = Document(docx_path)
            paragraph_count = 0
            
            # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
            self.special_formatted_text = []
            self.format_statistics = defaultdict(int)
            self.paragraph_formatting = []  # é‡ç½®æ®µè½æ ¼å¼ä¿¡æ¯
            
            for para in doc.paragraphs:
                paragraph_count += 1
                
                # åˆ†ææ®µè½æ ¼å¼ï¼ˆé¦–è¡Œç¼©è¿›ç­‰ï¼‰
                para_formats = self._analyze_paragraph_formatting(para, paragraph_count)
                if para_formats:
                    # ä¿å­˜æ®µè½æ ¼å¼ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ®µè½æ–‡æœ¬
                    para_text = para.text.strip()
                    if para_text:  # åªä¿å­˜éç©ºæ®µè½
                        self.paragraph_formatting.append({
                            'paragraph_index': paragraph_count,
                            'text': para_text,
                            'formats': para_formats,
                            'has_first_line_indent': any('é¦–è¡Œç¼©è¿›' in fmt for fmt in para_formats),
                            'is_centered': any('å¯¹é½: å±…ä¸­' in fmt for fmt in para_formats),
                            'is_right_aligned': any('å¯¹é½: å³å¯¹é½' in fmt for fmt in para_formats)
                        })
                
                # åˆ†ææ®µè½ä¸­çš„æ–‡æœ¬æ ¼å¼
                for run_index, run in enumerate(para.runs):
                    self._analyze_text_formatting(run, paragraph_count, run_index)
            
            # åˆ†æè¡¨æ ¼
            table_count = 0
            for table in doc.tables:
                table_count += 1
                for row_index, row in enumerate(table.rows):
                    for cell_index, cell in enumerate(row.cells):
                        for para in cell.paragraphs:
                            for run_index, run in enumerate(para.runs):
                                location = f"è¡¨æ ¼{table_count}-è¡Œ{row_index}-åˆ—{cell_index}"
                                self._analyze_text_formatting(run, location, run_index)
            
            # ç»Ÿè®¡é¦–è¡Œç¼©è¿›æ®µè½ã€å±…ä¸­æ®µè½å’Œå±…å³æ®µè½
            indent_paragraphs = len([p for p in self.paragraph_formatting if p['has_first_line_indent']])
            centered_paragraphs = len([p for p in self.paragraph_formatting if p['is_centered']])
            right_aligned_paragraphs = len([p for p in self.paragraph_formatting if p['is_right_aligned']])
            
            print(f"âœ… æ ¼å¼åˆ†æå®Œæˆ: {paragraph_count}ä¸ªæ®µè½, {table_count}ä¸ªè¡¨æ ¼")
            print(f"ğŸ“Š å‘ç° {len(self.special_formatted_text)} ä¸ªåŒ…å«ç‰¹æ®Šæ ¼å¼çš„æ–‡æœ¬ç‰‡æ®µ")
            print(f"ğŸ“ å‘ç° {indent_paragraphs} ä¸ªåŒ…å«é¦–è¡Œç¼©è¿›çš„æ®µè½")
            print(f"ğŸ“ å‘ç° {centered_paragraphs} ä¸ªå±…ä¸­å¯¹é½çš„æ®µè½")
            print(f"ğŸ“‘ å‘ç° {right_aligned_paragraphs} ä¸ªå±…å³å¯¹é½çš„æ®µè½")
            
            return {
                'total_paragraphs': paragraph_count,
                'total_tables': table_count,
                'special_format_count': len(self.special_formatted_text),
                'paragraph_format_count': len(self.paragraph_formatting),
                'indent_paragraph_count': indent_paragraphs,
                'centered_paragraph_count': centered_paragraphs,
                'right_aligned_paragraph_count': right_aligned_paragraphs,
                'format_statistics': dict(self.format_statistics)
            }
            
        except Exception as e:
            print(f"âŒ æ ¼å¼åˆ†æå¤±è´¥: {e}")
            return None
    
    # def _save_format_analysis(self, format_analysis, file_path):
    #     """ä¿å­˜æ ¼å¼åˆ†æç»“æœ - å·²ç§»é™¤"""
    #     try:
    #         # åˆ›å»ºæ ¼å¼åˆ†æç»“æœç›®å½•
    #         format_dir = Path("format_analysis")
    #         format_dir.mkdir(exist_ok=True)
    #         
    #         doc_name = Path(file_path).stem
    #         timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    #         
    #         # ä¿å­˜æ ¼å¼ç»Ÿè®¡æ‘˜è¦
    #         summary_file = format_dir / f"format_summary_{doc_name}_{timestamp}.txt"
    #         with open(summary_file, 'w', encoding='utf-8') as f:
    #             f.write(f"æ–‡æ¡£æ ¼å¼åˆ†ææŠ¥å‘Š\n")
    #             f.write(f"æ–‡æ¡£: {file_path}\n")
    #             f.write(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    #             f.write(f"=" * 50 + "\n\n")
    #             
    #             f.write("æ ¼å¼ç»Ÿè®¡:\n")
    #             for fmt, count in format_analysis['format_statistics'].items():
    #                 f.write(f"  {fmt}: {count}æ¬¡\n")
    #             
    #             f.write(f"\nç‰¹æ®Šæ ¼å¼æ–‡æœ¬è¯¦æƒ…:\n")
    #             for item in self.special_formatted_text:
    #                 f.write(f"\nä½ç½®{item['paragraph']}-ç‰‡æ®µ{item['run']}: \"{item['text'][:100]}{'...' if len(item['text']) > 100 else ''}\"\n")
    #                 for fmt in item['formats']:
    #                     f.write(f"  â””â”€ {fmt}\n")
    #         
    #         print(f"ğŸ“‹ æ ¼å¼åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {summary_file}")
    #         
    #         # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æ•°æ®
    #         json_file = format_dir / f"format_details_{doc_name}_{timestamp}.json"
    #         with open(json_file, 'w', encoding='utf-8') as f:
    #             json.dump({
    #                 'document_path': file_path,
    #                 'analysis_summary': format_analysis,
    #                 'special_formatted_text': self.special_formatted_text,
    #                 'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    #             }, f, ensure_ascii=False, indent=2)
    #         
    #         print(f"ğŸ“„ è¯¦ç»†æ ¼å¼æ•°æ®å·²ä¿å­˜: {json_file}")
    #         
    #     except Exception as e:
    #         print(f"âš ï¸ ä¿å­˜æ ¼å¼åˆ†æç»“æœå¤±è´¥: {e}")
    #     pass
    
    # def _integrate_format_info(self, api_result, format_analysis, file_path):
    #     """å°†æ ¼å¼ä¿¡æ¯æ•´åˆåˆ°APIç»“æœä¸­ - å·²ç§»é™¤"""
    #     try:
    #         # ç±»å‹æ£€æŸ¥
    #         if not isinstance(api_result, list):
    #             print(f"âš ï¸ APIç»“æœç±»å‹é”™è¯¯ï¼Œé¢„æœŸä¸ºlistï¼Œå®é™…ä¸º{type(api_result).__name__}ï¼Œè·³è¿‡æ ¼å¼ä¿¡æ¯æ•´åˆ")
    #             return
    #         
    #         # åˆ›å»ºæ ¼å¼ä¿¡æ¯æ¦‚è¦
    #         format_summary = {
    #             'total_special_formats': len(self.special_formatted_text),
    #             'format_types': list(format_analysis['format_statistics'].keys()),
    #             'analysis_enabled': True
    #         }
    #         
    #         # ä¸ºæ¯ä¸ªé¢˜ç›®æ•´åˆæ ¼å¼ä¿¡æ¯
    #         for question in api_result:
    #             if not isinstance(question, dict):
    #                 continue
    #                 
    #             # æ·»åŠ æ ¼å¼ä¿¡æ¯
    #             question['format_info'] = format_summary.copy()
    #             question['special_formats_in_question'] = []
    #             
    #             # åŒ¹é…ç‰¹æ®Šæ ¼å¼æ–‡æœ¬åˆ°å½“å‰é¢˜ç›®
    #             question_text = question.get('question', {}).get('content', '')
    #             for format_item in self.special_formatted_text:
    #                 if format_item['text'].strip() in question_text:
    #                     question['special_formats_in_question'].append({
    #                         'text': format_item['text'],
    #                         'formats': format_item['formats']
    #                     })
    #         
    #         print("ğŸ”— æ ¼å¼ä¿¡æ¯å·²æ•´åˆåˆ°APIç»“æœä¸­")
    #         
    #     except Exception as e:
    #         print(f"âš ï¸ æ•´åˆæ ¼å¼ä¿¡æ¯å¤±è´¥: {e}")
    #     pass
    
    def get_special_format_summary(self):
        """è·å–ç‰¹æ®Šæ ¼å¼æ‘˜è¦ä¿¡æ¯"""
        if not self.format_detection_enabled:
            return "æ ¼å¼æ£€æµ‹åŠŸèƒ½æœªå¯ç”¨"
        
        summary_lines = []
        
        # ç»Ÿè®¡é¦–è¡Œç¼©è¿›æ®µè½ã€å±…ä¸­æ®µè½å’Œå±…å³æ®µè½
        if hasattr(self, 'paragraph_formatting') and self.paragraph_formatting:
            indent_count = len([p for p in self.paragraph_formatting if p['has_first_line_indent']])
            centered_count = len([p for p in self.paragraph_formatting if p['is_centered']])
            right_aligned_count = len([p for p in self.paragraph_formatting if p['is_right_aligned']])
            summary_lines.append(f"ğŸ“ é¦–è¡Œç¼©è¿›æ®µè½: {indent_count} ä¸ª")
            summary_lines.append(f"ğŸ“ å±…ä¸­å¯¹é½æ®µè½: {centered_count} ä¸ª")
            summary_lines.append(f"ğŸ“‘ å±…å³å¯¹é½æ®µè½: {right_aligned_count} ä¸ª")
        
        # ç»Ÿè®¡ç‰¹æ®Šæ ¼å¼æ–‡æœ¬
        if self.special_formatted_text:
            # ç»Ÿè®¡é‡ç‚¹æ ¼å¼ï¼ˆå¸¦âš ï¸æ ‡è®°ï¼‰
            format_counts = defaultdict(int)
            for item in self.special_formatted_text:
                for fmt in item['formats']:
                    if 'âš ï¸' in fmt:
                        format_counts[fmt] += 1
            
            summary_lines.append(f"ğŸ“Š ç‰¹æ®Šæ ¼å¼æ–‡æœ¬: {len(self.special_formatted_text)} ä¸ªç‰‡æ®µ")
            summary_lines.extend(f"  {fmt}: {count}æ¬¡" 
                              for fmt, count in sorted(format_counts.items(), key=lambda x: x[1], reverse=True))
        
        if not summary_lines:
            return "æœªå‘ç°ç‰¹æ®Šæ ¼å¼æˆ–é¦–è¡Œç¼©è¿›"
        
        return "\n".join(summary_lines)
    
    def get_supported_formats(self):
        """è·å–æ”¯æŒçš„æ–‡æ¡£æ ¼å¼"""
        return ['.docx', '.doc', '.rtf', '.odt', '.txt']
    
    def is_supported_format(self, file_path):
        """æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ"""
        file_ext = Path(file_path).suffix.lower()
        return file_ext in self.get_supported_formats()
    
    # æ³¨é‡Šæ‰å›¾ç‰‡æå–åŠŸèƒ½ä»¥æé«˜è¿è¡Œæ•ˆç‡
    def extract_images_from_docx(self, docx_path, save_images=False):
        """ä»docxæ–‡ä»¶ä¸­æå–å›¾ç‰‡ - å·²æ³¨é‡Šä»¥æé«˜è¿è¡Œæ•ˆç‡"""
        print(f"ğŸ–¼ï¸  å›¾ç‰‡å¤„ç†å·²ç¦ç”¨ä»¥æé«˜è¿è¡Œæ•ˆç‡")
        return []
        
        # print(f"ğŸ–¼ï¸  ä»docxæ–‡ä»¶ä¸­æå–å›¾ç‰‡...")
        # 
        # images = []
        # if save_images:
        #     # åˆ›å»ºmediaæ–‡ä»¶å¤¹
        #     media_dir = Path("media")
        #     media_dir.mkdir(exist_ok=True)
        #     print(f"ğŸ“ åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•: {media_dir}")
        # 
        # try:
        #     # docxæ–‡ä»¶æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªzipæ–‡ä»¶
        #     with zipfile.ZipFile(docx_path, 'r') as zip_file:
        #         # æŸ¥æ‰¾mediaæ–‡ä»¶å¤¹ä¸­çš„å›¾ç‰‡
        #         for file_info in zip_file.filelist:
        #             if file_info.filename.startswith('word/media/'):
        #                 file_name = file_info.filename.split('/')[-1]
        #                 file_ext = Path(file_name).suffix.lower()
        #                 
        #                 # åªå¤„ç†å›¾ç‰‡æ–‡ä»¶
        #                 if file_ext in ['.png', '.jpg', '.jpeg', '.gif', '.bmp']:
        #                     try:
        #                         # è¯»å–å›¾ç‰‡æ•°æ®
        #                         with zip_file.open(file_info.filename) as img_file:
        #                             img_data = img_file.read()
        #                         
        #                         # è½¬æ¢ä¸ºPIL Imageå¯¹è±¡
        #                         img = Image.open(BytesIO(img_data))
        #                         
        #                         # å¦‚æœéœ€è¦ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°
        #                         if save_images:
        #                             img_path = media_dir / file_name
        #                             with open(img_path, 'wb') as f:
        #                                 f.write(img_data)
        #                             print(f"  ğŸ’¾ ä¿å­˜å›¾ç‰‡: {img_path}")
        #                         
        #                         images.append({
        #                             'filename': file_name,
        #                             'path': file_info.filename,
        #                             'image': img,
        #                             'data': img_data,
        #                             'size': img.size,
        #                             'format': img.format
        #                         })
        #                         
        #                         print(f"  ğŸ“· æå–å›¾ç‰‡: {file_name} ({img.size[0]}x{img.size[1]})")
        #                         
        #                     except Exception as e:
        #                         print(f"  âš ï¸  å›¾ç‰‡ {file_name} è¯»å–å¤±è´¥: {e}")
        #                         continue
        #         
        #         print(f"âœ… å…±æå– {len(images)} å¼ å›¾ç‰‡")
        #         return images
        #         
        # except Exception as e:
        #     print(f"âŒ æå–å›¾ç‰‡å¤±è´¥: {e}")
        #     return []
    
    # æ³¨é‡Šæ‰LLMå›¾ç‰‡åˆ†æåŠŸèƒ½ä»¥æé«˜è¿è¡Œæ•ˆç‡
    def analyze_image_with_llm(self, image_data, image_name):
        """ä½¿ç”¨LLMåˆ†æå›¾ç‰‡å†…å®¹ - å·²æ³¨é‡Šä»¥æé«˜è¿è¡Œæ•ˆç‡"""
        print(f"  ğŸ¤– å›¾ç‰‡åˆ†æå·²ç¦ç”¨ä»¥æé«˜è¿è¡Œæ•ˆç‡: {image_name}")
        return f"å›¾ç‰‡å†…å®¹: {image_name}"
        
        # print(f"  ğŸ¤– ä½¿ç”¨LLMåˆ†æå›¾ç‰‡: {image_name}")
        # 
        # try:
        #     # å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64
        #     img_buffer = BytesIO()
        #     if isinstance(image_data, bytes):
        #         # å¦‚æœå·²ç»æ˜¯bytesï¼Œç›´æ¥ä½¿ç”¨
        #         img_base64 = base64.b64encode(image_data).decode()
        #     else:
        #         # å¦‚æœæ˜¯PIL Imageï¼Œå…ˆä¿å­˜ä¸ºbytes
        #         image_data.save(img_buffer, format='PNG')
        #         img_base64 = base64.b64encode(img_buffer.getvalue()).decode()
        #     
        #     # æ„å»ºAPIè¯·æ±‚
        #     headers = {
        #         "Authorization": f"Bearer {self.api_key}",
        #         "Content-Type": "application/json"
        #     }
        #     
        #     # ä½¿ç”¨vision APIåˆ†æå›¾ç‰‡
        #     data = {
        #         "model": "doubao-seed-1-6-250615",
        #         "messages": [
        #             {
        #                 "role": "user",
        #                 "content": [
        #                     {
        #                         "type": "text",
        #                         "text": "è¯·åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼Œç”¨ç®€æ´çš„ä¸­æ–‡æè¿°å›¾ç‰‡ä¸­æ˜¾ç¤ºçš„å†…å®¹ã€‚å¦‚æœæ˜¯è¯•å·é¢˜ç›®ï¼Œè¯·æè¿°é¢˜ç›®ç±»å‹å’Œä¸»è¦å†…å®¹ã€‚"
        #                     },
        #                     {
        #                         "type": "image_url",
        #                         "image_url": {
        #                             "url": f"data:image/png;base64,{img_base64}"
        #                         }
        #                     }
        #                 ]
        #             }
        #         ],
        #         "max_tokens": 500,
        #         "temperature": 0.1
        #     }
        #     
        #     # è°ƒç”¨API
        #     response = requests.post(
        #         f"{self.base_url}/v3/chat/completions",
        #         headers=headers,
        #         json=data,
        #         timeout=60
        #     )
        #     
        #     if response.status_code == 200:
        #         result = response.json()
        #         if 'choices' in result and len(result['choices']) > 0:
        #             content = result['choices'][0]['message']['content']
        #             print(f"  âœ… å›¾ç‰‡åˆ†æå®Œæˆ: {content[:100]}...")
        #             return content
        #         else:
        #             print(f"  âš ï¸  APIå“åº”æ ¼å¼å¼‚å¸¸")
        #             return f"å›¾ç‰‡å†…å®¹: {image_name}"
        #     else:
        #         print(f"  âŒ APIè°ƒç”¨å¤±è´¥: {response.status_code}")
        #         return f"å›¾ç‰‡å†…å®¹: {image_name}"
        #         
        # except Exception as e:
        #     print(f"  âŒ å›¾ç‰‡åˆ†æå¼‚å¸¸: {e}")
        #     return f"å›¾ç‰‡å†…å®¹: {image_name}"
    
    # æ³¨é‡Šæ‰å›¾ç‰‡æ°´å°æ›¿æ¢åŠŸèƒ½ä»¥æé«˜è¿è¡Œæ•ˆç‡
    def replace_image_watermarks(self, content, images):
        """æ›¿æ¢å†…å®¹ä¸­çš„å›¾ç‰‡æ°´å°ä¸ºå›¾ç‰‡å†…å®¹æè¿° - å·²æ³¨é‡Šä»¥æé«˜è¿è¡Œæ•ˆç‡"""
        print("ğŸ”„ å›¾ç‰‡æ°´å°æ›¿æ¢å·²ç¦ç”¨ä»¥æé«˜è¿è¡Œæ•ˆç‡")
        return content
        
        # print("ğŸ”„ æ›¿æ¢å›¾ç‰‡æ°´å°...")
        # 
        # if not images:
        #     print("  â„¹ï¸  æ²¡æœ‰å›¾ç‰‡éœ€è¦å¤„ç†")
        #     return content
        # 
        # # æŸ¥æ‰¾å›¾ç‰‡å¼•ç”¨æ¨¡å¼
        # # åŒ¹é…ç±»ä¼¼ ![å­¦ç§‘ç½‘(www.zxxk.com)--æ•™è‚²èµ„æºé—¨æˆ·...](media/image6.png) çš„æ¨¡å¼
        # image_pattern = r'!\[([^\]]+)\]\(([^)]+)\)'
        # 
        # def replace_image_ref(match):
        #     watermark_text = match.group(1)
        #     image_path = match.group(2)
        #     
        #     # æå–å›¾ç‰‡æ–‡ä»¶å
        #     image_filename = Path(image_path).name
        #     
        #     # æŸ¥æ‰¾å¯¹åº”çš„å›¾ç‰‡
        #     for img_info in images:
        #         if img_info['filename'] == image_filename:
        #             # ä½¿ç”¨LLMåˆ†æå›¾ç‰‡å†…å®¹
        #             image_description = self.analyze_image_with_llm(img_info['image'], image_filename)
        #             
        #             # æ›¿æ¢æ°´å°æ–‡å­—ä¸ºå›¾ç‰‡æè¿°
        #             new_text = f"![{image_description}]({image_path})"
        #             print(f"  ğŸ”„ æ›¿æ¢: {watermark_text[:50]}... -> {image_description[:50]}...")
        #             return new_text
        #     
        #     # å¦‚æœæ²¡æ‰¾åˆ°å¯¹åº”å›¾ç‰‡ï¼Œä¿ç•™åŸæ ·
        #     print(f"  âš ï¸  æœªæ‰¾åˆ°å›¾ç‰‡: {image_filename}")
        #     return match.group(0)
        # 
        # # æ‰§è¡Œæ›¿æ¢
        # modified_content = re.sub(image_pattern, replace_image_ref, content)
        # 
        # # ç»Ÿè®¡æ›¿æ¢æ¬¡æ•°
        # original_count = len(re.findall(image_pattern, content))
        # modified_count = len(re.findall(image_pattern, modified_content))
        # 
        # print(f"âœ… æ°´å°æ›¿æ¢å®Œæˆï¼Œå¤„ç†äº† {len(images)} å¼ å›¾ç‰‡")
        # return modified_content
    
    def convert_word_to_text(self, file_path, output_format='markdown'):
        """ä½¿ç”¨pandocå°†Wordæ–‡æ¡£è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œå¹¶å¢å¼ºæ ¼å¼æ ‡æ³¨"""
        if not self.pandoc_available:
            print("âŒ Pandocä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†æ–‡æ¡£")
            return None
        
        if not self.is_supported_format(file_path):
            print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {Path(file_path).suffix}")
            print(f"æ”¯æŒçš„æ ¼å¼: {', '.join(self.get_supported_formats())}")
            return None
        
        if not os.path.exists(file_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None
        
        print(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")
        print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(file_path) / (1024*1024):.2f} MB")
        
        try:
            # æ„å»ºpandocå‘½ä»¤
            cmd = [
                'pandoc',
                file_path,
                '--to', output_format,
                '--wrap', 'none',  # ä¸è‡ªåŠ¨æ¢è¡Œ
                '--standalone',  # ç”Ÿæˆç‹¬ç«‹æ–‡æ¡£
                '--quiet'  # å‡å°‘è¾“å‡º
            ]
            
            print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            # æ‰§è¡Œpandocè½¬æ¢
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                content = result.stdout
                print(f"âœ… è½¬æ¢æˆåŠŸ: {len(content)} å­—ç¬¦")
                
                # å¦‚æœæ˜¯docxæ–‡ä»¶ï¼Œæå–å›¾ç‰‡å¹¶æ›¿æ¢æ°´å° - å·²æ³¨é‡Šä»¥æé«˜è¿è¡Œæ•ˆç‡
                if file_path.lower().endswith('.docx'):
                     print("æ£€æµ‹åˆ°docxæ–‡ä»¶ï¼Œå›¾ç‰‡å¤„ç†å·²ç¦ç”¨ä»¥æé«˜è¿è¡Œæ•ˆç‡")
                     # images = self.extract_images_from_docx(file_path, save_images=True)
                     # if images:
                     #     content = self.replace_image_watermarks(content, images)
                     # else:
                     #     print("æœªæ‰¾åˆ°å›¾ç‰‡æˆ–å›¾ç‰‡å¤„ç†å¤±è´¥")
                
                # æ–°å¢ï¼šå¦‚æœæœ‰æ ¼å¼åˆ†æç»“æœï¼Œå¢å¼ºpandocå†…å®¹
                if hasattr(self, 'special_formatted_text') and self.special_formatted_text:
                    print("ğŸ¨ å¼€å§‹å¢å¼ºæ ¼å¼æ ‡æ³¨...")
                    content = self._enhance_content_with_format_info(content)
                
                # ä¿å­˜è½¬æ¢ç»“æœ
                pandoc_res_dir = Path("pandoc_res")
                pandoc_res_dir.mkdir(exist_ok=True)
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_filename = pandoc_res_dir / f"pandocè½¬æ¢ç»“æœ_{timestamp}.txt"
                with open(output_filename, 'w', encoding='utf-8') as f:
                    f.write(content)
                print(f"è½¬æ¢ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")
                
                return content
            else:
                print(f"âŒ è½¬æ¢å¤±è´¥: {result.stderr}")
                return None
                
        except subprocess.TimeoutExpired:
            print("âŒ è½¬æ¢è¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰")
            return None
        except Exception as e:
            print(f"âŒ è½¬æ¢å¼‚å¸¸: {e}")
            return None
    
    def _clean_dot_below_markers(self, text):
        """æ¸…ç†åŠ ç‚¹å­—æ ‡è®°ï¼Œç”¨äºåŒ¹é…æ¯”è¾ƒ"""
        import re
        
        # æ ¼å¼1ï¼šå®Œæ•´pandocæ ¼å¼ [\[DOT_BELOW\]å­—ç¬¦\[/DOT_BELOW\]]{.underline}
        pattern1 = r'\[\\\[DOT_BELOW\\\]([\u4e00-\u9fff]+)\\\[/DOT_BELOW\\\]\]\{\.underline\}'
        cleaned = re.sub(pattern1, r'\1', text)
        
        # æ ¼å¼2ï¼šç®€åŒ–æ ¼å¼ [DOT_BELOW]å­—ç¬¦[/DOT_BELOW] (æ”¯æŒå¤šä¸ªå­—ç¬¦)
        pattern2 = r'\[DOT_BELOW\]([\u4e00-\u9fff]+)\[/DOT_BELOW\]'
        cleaned = re.sub(pattern2, r'\1', cleaned)
        
        # æ ¼å¼3ï¼šå¤„ç†ä¸å®Œæ•´çš„DOT_BELOWæ ‡è®°ï¼ˆå¦‚æˆªæ–­çš„æ–‡æœ¬ï¼‰
        pattern3 = r'\[DOT_BELOW\]([\u4e00-\u9fff]*)\[/DOT.*?'
        cleaned = re.sub(pattern3, r'\1', cleaned)
        
        # æ ¼å¼4ï¼šæ¸…ç†å‰©ä½™çš„DOT_BELOWå¼€å§‹æ ‡è®°
        pattern4 = r'\[DOT_BELOW\]'
        cleaned = re.sub(pattern4, '', cleaned)
        
        return cleaned
    
    def _should_enable_detailed_debug(self, para_text):
        """
        é€šç”¨è°ƒè¯•æ¡ä»¶åˆ¤æ–­ï¼šåŸºäºæ–‡æœ¬ç‰¹å¾å’Œå¤æ‚åº¦å†³å®šæ˜¯å¦å¯ç”¨è¯¦ç»†è°ƒè¯•
        """
        # æ–‡æœ¬é•¿åº¦ç›¸å…³æ¡ä»¶
        text_length = len(para_text)
        if text_length < 5:  # è¿‡çŸ­æ–‡æœ¬é€šå¸¸ä¸éœ€è¦è¯¦ç»†è°ƒè¯•
            return False
        if text_length > 50:  # é•¿æ–‡æœ¬æ›´éœ€è¦è°ƒè¯•
            return True
            
        # åŒ…å«ç‰¹æ®Šæ ¼å¼æ ‡è®°
        special_markers = ['DOT_BELOW', 'ã€', 'ã€‘', '[', ']', '\\[', '\\]']
        if any(marker in para_text for marker in special_markers):
            return True
            
        # åŒ…å«å¤æ‚æ ‡ç‚¹æˆ–æ ¼å¼
        complex_chars = ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©', 
                        'â€œ', 'â€', 'â€˜', 'â€™', 'ï¼ˆ', 'ï¼‰', 'â€”â€”', 'â€¦']
        if any(char in para_text for char in complex_chars):
            return True
            
        # åŒ…å«å¼•å·æˆ–ç‰¹æ®Šç¬¦å·
        if 'â€œ' in para_text or "â€" in para_text or 'ã€Œ' in para_text or 'ã€' in para_text:
            return True
            
        # ä¸­ç­‰é•¿åº¦çš„æ–‡æœ¬ï¼Œæœ‰ä¸€å®šè°ƒè¯•ä»·å€¼
        if 20 <= text_length <= 50:
            return True
            
        return False
    
    def _has_high_text_similarity(self, text1, text2):
        """
        é€šç”¨æ–‡æœ¬ç›¸ä¼¼åº¦åˆ¤æ–­ï¼šåŸºäºå¤šç§æŒ‡æ ‡è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
        """
        if not text1 or not text2:
            return False
            
        # é•¿åº¦ç›¸ä¼¼æ€§æ£€æŸ¥
        len1, len2 = len(text1), len(text2)
        if abs(len1 - len2) > max(len1, len2) * 0.5:  # é•¿åº¦å·®å¼‚è¶…è¿‡50%
            return False
            
        # å­—ç¬¦é›†é‡å æ£€æŸ¥
        chars1, chars2 = set(text1), set(text2)
        overlap = len(chars1 & chars2)
        union = len(chars1 | chars2)
        if union > 0:
            char_similarity = overlap / union
            if char_similarity > 0.7:  # å­—ç¬¦é‡å åº¦è¶…è¿‡70%
                return True
                
        # å­ä¸²åŒ…å«æ£€æŸ¥
        shorter, longer = (text1, text2) if len1 < len2 else (text2, text1)
        if len(shorter) >= 5:  # åªå¯¹æœ‰æ„ä¹‰é•¿åº¦çš„æ–‡æœ¬åšå­ä¸²æ£€æŸ¥
            # æ£€æŸ¥è¾ƒçŸ­æ–‡æœ¬çš„å‰åŠéƒ¨åˆ†æ˜¯å¦åœ¨è¾ƒé•¿æ–‡æœ¬ä¸­
            half_len = len(shorter) // 2
            if half_len >= 3 and shorter[:half_len] in longer:
                return True
            # æ£€æŸ¥è¾ƒçŸ­æ–‡æœ¬çš„ååŠéƒ¨åˆ†æ˜¯å¦åœ¨è¾ƒé•¿æ–‡æœ¬ä¸­    
            if half_len >= 3 and shorter[-half_len:] in longer:
                return True
                
        # ä¸­æ–‡è¯æ±‡é‡å æ£€æŸ¥ï¼ˆé’ˆå¯¹ä¸­æ–‡æ–‡æ¡£ï¼‰
        import re
        chinese_words1 = re.findall(r'[\u4e00-\u9fff]{2,}', text1)
        chinese_words2 = re.findall(r'[\u4e00-\u9fff]{2,}', text2)
        
        if chinese_words1 and chinese_words2:
            word_overlap = len(set(chinese_words1) & set(chinese_words2))
            word_total = len(set(chinese_words1) | set(chinese_words2))
            if word_total > 0 and word_overlap / word_total > 0.5:  # è¯æ±‡é‡å è¶…è¿‡50%
                return True
                
        return False
    
    def _normalize_quotes(self, text):
        """æ ‡å‡†åŒ–å¼•å·ï¼Œç”¨äºåŒ¹é…æ¯”è¾ƒ"""
        # å°†å„ç§ä¸­æ–‡å¼•å·ç»Ÿä¸€ä¸ºæ ‡å‡†å¼•å·
        quote_mappings = {
            'â€œ': '"',  # å·¦åŒå¼•å· (8220) -> æ™®é€šåŒå¼•å· (34)
            'â€': '"',  # å³åŒå¼•å· (8221) -> æ™®é€šåŒå¼•å· (34)
            'â€˜': "'",  # å·¦å•å¼•å· (8216) -> æ™®é€šå•å¼•å· (39)
            'â€™': "'",  # å³å•å¼•å· (8217) -> æ™®é€šå•å¼•å· (39)
            'ã€Œ': '"',  # æ—¥å¼å·¦å¼•å·
            'ã€': '"',  # æ—¥å¼å³å¼•å·
            'ã€': '"',  # æ—¥å¼å·¦åŒå¼•å·
            'ã€': '"',  # æ—¥å¼å³åŒå¼•å·
        }
        
        result = text
        for old_quote, new_quote in quote_mappings.items():
            result = result.replace(old_quote, new_quote)
        return result
    
    def _find_best_match_in_content(self, para_text, content):
        """åœ¨å†…å®¹ä¸­æ‰¾åˆ°æ®µè½çš„æœ€ä½³åŒ¹é…ä½ç½®"""
        
        # æ·»åŠ ç©ºæ ¼å¤„ç†
        para_text_cleaned = ' '.join(para_text.split())
        
        # ç‰¹æ®Šå¤„ç†ï¼šä¼˜å…ˆå°è¯•åŒ¹é…ç‹¬ç«‹è¡Œ
        lines = content.split('\n')
        for line in lines:
            line_stripped = line.strip()
            line_cleaned = ' '.join(line_stripped.split())
            
            # å°è¯•ç›´æ¥åŒ¹é…
            if line_stripped == para_text or line_cleaned == para_text_cleaned:
                return line_stripped, "ç‹¬ç«‹è¡Œ"
            
            # å°è¯•æ ‡å‡†åŒ–ååŒ¹é…ç‹¬ç«‹è¡Œ
            normalized_line = self._normalize_quotes(line_stripped)
            normalized_para = self._normalize_quotes(para_text)
            normalized_line_cleaned = ' '.join(normalized_line.split())
            normalized_para_cleaned = ' '.join(normalized_para.split())
            
            if normalized_line_cleaned == normalized_para_cleaned:
                return line_stripped, "ç‹¬ç«‹è¡Œå¼•å·æ¸…ç†"
            
            # ğŸ”§ æ–°å¢ï¼šDOT_BELOWæ¸…ç†ååŒ¹é…
            cleaned_line = self._clean_dot_below_markers(line_stripped)
            cleaned_para = self._clean_dot_below_markers(para_text)
            if cleaned_line == cleaned_para:
                return line_stripped, "ç‹¬ç«‹è¡ŒDOT_BELOWæ¸…ç†"
            
            # ç»¼åˆå¤„ç†ï¼šDOT_BELOW + å¼•å· + ç©ºæ ¼
            both_cleaned_line = ' '.join(self._normalize_quotes(self._clean_dot_below_markers(line_stripped)).split())
            both_cleaned_para = ' '.join(self._normalize_quotes(self._clean_dot_below_markers(para_text)).split())
            if both_cleaned_line == both_cleaned_para:
                return line_stripped, "ç‹¬ç«‹è¡Œç»¼åˆæ¸…ç†"
        
        # ä¼˜åŒ–é•¿åº¦ç­–ç•¥ - å¯¹çŸ­æ–‡æœ¬æ›´çµæ´»
        if len(para_text) <= 8:
            # çŸ­æ–‡æœ¬ï¼šä¼˜å…ˆå®Œæ•´åŒ¹é…ï¼Œç„¶åé€æ­¥å‡å°‘
            lengths = [len(para_text)]
            if len(para_text) > 3:
                lengths.extend([len(para_text) - 1, len(para_text) - 2])
            if len(para_text) > 5:
                lengths.append(5)
        else:
            # é•¿æ–‡æœ¬ï¼šä½¿ç”¨æ›´å¤šé€‰é¡¹
            lengths = [25, 20, 15, 12, 10, 8]
        
        for length in lengths:
            if len(para_text) < length:
                continue
                
            para_start = para_text[:length]
            para_start_cleaned = ' '.join(para_start.split())
            
            # æ–¹æ³•1ï¼šç›´æ¥åŒ¹é…
            if para_start in content:
                if not any(f"ã€{marker}ã€‘{para_start}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                    return para_start, f"ç²¾ç¡®{length}"
            
            # æ–¹æ³•1.5ï¼šç©ºæ ¼æ¸…ç†ååŒ¹é…
            if para_start_cleaned != para_start and para_start_cleaned in content:
                if not any(f"ã€{marker}ã€‘{para_start_cleaned}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                    return para_start, f"ç©ºæ ¼æ¸…ç†{length}"
            
            # æ–¹æ³•2ï¼šæ ‡å‡†åŒ–å¼•å·ååŒ¹é…
            normalized_para_start = self._normalize_quotes(para_start)
            if normalized_para_start != para_start:
                if normalized_para_start in content:
                    if not any(f"ã€{marker}ã€‘{normalized_para_start}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                        return normalized_para_start, f"å¼•å·{length}"
                
                # åŒæ—¶æ ‡å‡†åŒ–å¼•å·å’Œæ¸…ç†ç©ºæ ¼
                normalized_cleaned = ' '.join(normalized_para_start.split())
                if normalized_cleaned != normalized_para_start and normalized_cleaned in content:
                    if not any(f"ã€{marker}ã€‘{normalized_cleaned}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                        return para_start, f"å¼•å·ç©ºæ ¼{length}"
            
            # æ–¹æ³•3ï¼šæ¸…ç†åŠ ç‚¹å­—æ ‡è®°ååŒ¹é…
            cleaned_para_start = self._clean_dot_below_markers(para_start)
            if cleaned_para_start != para_start:
                if cleaned_para_start in content:
                    if not any(f"ã€{marker}ã€‘{cleaned_para_start}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                        return cleaned_para_start, f"æ¸…ç†{length}"
                
                # ğŸ”§ æ–°å¢ï¼šå¦‚æœç›´æ¥åœ¨contentä¸­æ‰¾ä¸åˆ°ï¼Œå°è¯•é€è¡ŒåŒ¹é…
                lines = content.split('\n')
                for line in lines:
                    line_cleaned = self._clean_dot_below_markers(line.strip())
                    if cleaned_para_start in line_cleaned:
                        if not any(f"ã€{marker}ã€‘{cleaned_para_start}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                            return cleaned_para_start, f"é€è¡Œæ¸…ç†{length}"
            
            # æ–¹æ³•4ï¼šç»¼åˆå¤„ç†ï¼ˆå¼•å·+åŠ ç‚¹å­—+ç©ºæ ¼ï¼‰
            both_processed = self._normalize_quotes(self._clean_dot_below_markers(para_start))
            both_processed_cleaned = ' '.join(both_processed.split())
            
            if both_processed != para_start:
                if both_processed in content:
                    if not any(f"ã€{marker}ã€‘{both_processed}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                        return both_processed, f"ç»¼åˆ{length}"
                
                if both_processed_cleaned != both_processed and both_processed_cleaned in content:
                    if not any(f"ã€{marker}ã€‘{both_processed_cleaned}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                        return para_start, f"ç»¼åˆç©ºæ ¼{length}"
        
        # ç‰¹æ®Šå¤„ç†ï¼šåºå·æ®µè½ï¼ˆâ‘ â‘¡â‘¢â‘­ç­‰ï¼‰
        import re
        if re.match(r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³]', para_text):
            content_without_number = para_text[1:].strip()
            for attempt_length in [len(content_without_number), min(10, len(content_without_number))]:
                if len(content_without_number) >= attempt_length > 0:
                    text_to_find = content_without_number[:attempt_length]
                    if text_to_find in content:
                        if not any(f"ã€{marker}ã€‘{text_to_find}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                            return para_text, f"åºå·åŒ¹é…{attempt_length}"
        
        # ğŸ†• å›é€€åŒ¹é…ç­–ç•¥ï¼šæ›´å®½æ¾çš„åŒ¹é…ç®—æ³•
        # åªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        should_debug_fallback = self._should_enable_detailed_debug(para_text)
        if should_debug_fallback:
            print(f"     å°è¯•å›é€€åŒ¹é…ç­–ç•¥...")
        
        # å›é€€ç­–ç•¥1ï¼šæ¨¡ç³Šå­—ç¬¦åŒ¹é…
        para_chars = set(para_text)
        content_lines = content.split('\n')
        best_match = None
        best_score = 0
        
        for line in content_lines:
            line_stripped = line.strip()
            if len(line_stripped) < 5:  # è·³è¿‡è¿‡çŸ­çš„è¡Œ
                continue
                
            # è®¡ç®—å­—ç¬¦é‡å ç‡
            line_chars = set(line_stripped)
            overlap = len(para_chars & line_chars)
            total_chars = len(para_chars | line_chars)
            if total_chars > 0:
                score = overlap / total_chars
                
                # å¦‚æœé‡å ç‡å¾ˆé«˜ä¸”é•¿åº¦ç›¸è¿‘
                if score > 0.8 and abs(len(line_stripped) - len(para_text)) < max(5, len(para_text) * 0.2):
                    if score > best_score:
                        best_score = score
                        best_match = line_stripped
        
        if best_match:
            if should_debug_fallback:
                print(f"     âœ… æ¨¡ç³ŠåŒ¹é…æˆåŠŸ (ç›¸ä¼¼åº¦: {best_score:.2f})")
            return best_match, f"æ¨¡ç³ŠåŒ¹é…({best_score:.2f})"
        
        # å›é€€ç­–ç•¥2ï¼šå…³é”®è¯åŒ¹é…
        # æå–ä¸­æ–‡å­—ç¬¦ä½œä¸ºå…³é”®è¯
        chinese_chars = re.findall(r'[\u4e00-\u9fff]+', para_text)
        if chinese_chars:
            # å–æœ€é•¿çš„ä¸­æ–‡è¯æ±‡ä½œä¸ºå…³é”®è¯
            key_phrase = max(chinese_chars, key=len)
            if len(key_phrase) >= 3:  # è‡³å°‘3ä¸ªå­—
                for line in content_lines:
                    line_stripped = line.strip()
                    if key_phrase in line_stripped and len(line_stripped) > len(key_phrase):
                        # æ£€æŸ¥ä¸Šä¸‹æ–‡ç›¸ä¼¼æ€§
                        context_before = para_text[:para_text.find(key_phrase)]
                        context_after = para_text[para_text.find(key_phrase) + len(key_phrase):]
                        
                        line_key_pos = line_stripped.find(key_phrase)
                        line_before = line_stripped[:line_key_pos]
                        line_after = line_stripped[line_key_pos + len(key_phrase):]
                        
                        # ç®€å•çš„ä¸Šä¸‹æ–‡åŒ¹é…
                        before_match = any(c in line_before for c in context_before[-3:]) if context_before else True
                        after_match = any(c in line_after for c in context_after[:3]) if context_after else True
                        
                        if before_match and after_match:
                            if should_debug_fallback:
                                print(f"     âœ… å…³é”®è¯åŒ¹é…æˆåŠŸ (å…³é”®è¯: {key_phrase})")
                            return line_stripped, f"å…³é”®è¯åŒ¹é…({key_phrase})"
        
        # å›é€€ç­–ç•¥3ï¼šæ•°å­—åºå·åŒ¹é… 
        number_match = re.match(r'^([â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³]|[0-9]+[\.ã€])', para_text)
        if number_match:
            number_prefix = number_match.group(0)
            remaining_text = para_text[len(number_prefix):].strip()
            
            for line in content_lines:
                line_stripped = line.strip()
                if remaining_text and len(remaining_text) > 3 and remaining_text[:10] in line_stripped:
                    if should_debug_fallback:
                        print(f"     âœ… åºå·å†…å®¹åŒ¹é…æˆåŠŸ")
                    return line_stripped, "åºå·å†…å®¹åŒ¹é…"
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        escaped_text = re.escape(para_text[:min(5, len(para_text))])
        if re.search(escaped_text, content):
            return None, "éƒ¨åˆ†å­˜åœ¨ä½†æ— æ³•åŒ¹é…"
        
        if should_debug_fallback:
            print(f"     âŒ æ‰€æœ‰åŒ¹é…ç­–ç•¥éƒ½å¤±è´¥")
        return None, None
    
    def _enhance_content_with_format_info(self, content):
        """æ ¹æ®æ ¼å¼åˆ†æç»“æœå¢å¼ºpandocå†…å®¹"""
        print(f"ğŸ” å¼€å§‹åˆ†æ {len(self.special_formatted_text)} ä¸ªç‰¹æ®Šæ ¼å¼æ–‡æœ¬")
        print(f"ğŸ“ å¼€å§‹åˆ†æ {len(self.paragraph_formatting)} ä¸ªæ®µè½æ ¼å¼")
        
        # ğŸš¨ é‡è¦ï¼šä¼˜å…ˆå¤„ç†å±…å³æ®µè½ï¼Œé¿å…è¢«é¦–è¡Œç¼©è¿›è¯¯åŒ¹é…
        # ç¬¬ä¸€æ­¥ï¼šå¤„ç†å±…å³æ®µè½
        right_aligned_enhanced_count = 0
        for para_info in self.paragraph_formatting:
            if para_info['is_right_aligned']:
                para_text = para_info['text'].strip()
                print(f"ğŸ” æ£€æŸ¥å±…å³æ–‡æœ¬: \"{para_text}\" (é•¿åº¦: {len(para_text)})")
                
                # è·³è¿‡è¿‡çŸ­çš„æ–‡æœ¬
                if len(para_text) < 1:
                    continue
                
                # ä½¿ç”¨æ”¹è¿›çš„åŒ¹é…ç®—æ³•
                match_result, match_type = self._find_best_match_in_content(para_text, content)
                
                if match_result:
                    # åœ¨åŒ¹é…çš„æ–‡æœ¬å‰æ·»åŠ æ ‡è®°
                    enhanced_start = f"ã€å±…å³ã€‘{match_result}"
                    content = content.replace(match_result, enhanced_start, 1)
                    right_aligned_enhanced_count += 1
                    print(f"âœ… å±…å³æ ‡è®°({match_type}): \"{match_result[:30]}...\"")
                else:
                    print(f"âŒ å±…å³æœªåŒ¹é…: \"{para_text[:30]}...\" (é•¿åº¦: {len(para_text)})")
        
        # ç¬¬äºŒæ­¥ï¼šå¤„ç†å±…ä¸­æ®µè½
        centered_enhanced_count = 0
        for para_info in self.paragraph_formatting:
            if para_info['is_centered']:
                para_text = para_info['text'].strip()
                
                # è·³è¿‡è¿‡çŸ­çš„æ–‡æœ¬
                if len(para_text) < 2:
                    continue
                
                # é¿å…é‡å¤æ ‡è®°ï¼ˆå¦‚æœå·²ç»æœ‰å±…å³æ ‡è®°ï¼‰
                check_lengths = [min(10, len(para_text)), min(8, len(para_text))] if len(para_text) > 5 else [len(para_text)]
                if any(f"ã€å±…å³ã€‘{para_text[:length]}" in content for length in check_lengths if len(para_text) >= length):
                    print(f"  â†’ è·³è¿‡ï¼šå·²æœ‰å±…å³æ ‡è®°")
                    continue
                
                # ä½¿ç”¨æ”¹è¿›çš„åŒ¹é…ç®—æ³•
                match_result, match_type = self._find_best_match_in_content(para_text, content)
                
                if match_result:
                    # åœ¨åŒ¹é…çš„æ–‡æœ¬å‰æ·»åŠ æ ‡è®°
                    enhanced_start = f"ã€å±…ä¸­ã€‘{match_result}"
                    content = content.replace(match_result, enhanced_start, 1)
                    centered_enhanced_count += 1
                    print(f"âœ… å±…ä¸­æ ‡è®°({match_type}): \"{match_result[:30]}...\"")
                else:
                    print(f"âŒ å±…ä¸­æœªåŒ¹é…: \"{para_text[:20]}...\"")
        
        # ç¬¬ä¸‰æ­¥ï¼šå¤„ç†æ®µè½é¦–è¡Œç¼©è¿›ï¼ˆæœ€åå¤„ç†ï¼Œé¿å…è¯¯æŠ¢å…¶ä»–æ ¼å¼ï¼‰
        indent_enhanced_count = 0
        for para_info in self.paragraph_formatting:
            if para_info['has_first_line_indent']:
                para_text = para_info['text'].strip()
                
                # è·³è¿‡è¿‡çŸ­çš„æ–‡æœ¬
                if len(para_text) < 8:
                    continue
                
                # é¿å…é‡å¤æ ‡è®°ï¼ˆå¦‚æœå·²ç»æœ‰å…¶ä»–æ ‡è®°ï¼‰
                check_lengths = [min(10, len(para_text)), min(8, len(para_text))] if len(para_text) > 5 else [len(para_text)]
                if any(f"ã€å±…å³ã€‘{para_text[:length]}" in content for length in check_lengths if len(para_text) >= length):
                    print(f"  â†’ è·³è¿‡ï¼šå·²æœ‰å±…å³æ ‡è®°")
                    continue
                if any(f"ã€å±…ä¸­ã€‘{para_text[:length]}" in content for length in check_lengths if len(para_text) >= length):
                    print(f"  â†’ è·³è¿‡ï¼šå·²æœ‰å±…ä¸­æ ‡è®°")
                    continue
                
                # ä½¿ç”¨æ”¹è¿›çš„åŒ¹é…ç®—æ³•
                match_result, match_type = self._find_best_match_in_content(para_text, content)
                
                if match_result:
                    # åœ¨åŒ¹é…çš„æ–‡æœ¬å‰æ·»åŠ æ ‡è®°
                    enhanced_start = f"ã€é¦–è¡Œç¼©è¿›ã€‘{match_result}"
                    content = content.replace(match_result, enhanced_start, 1)
                    indent_enhanced_count += 1
                    print(f"âœ… ç¼©è¿›æ ‡è®°({match_type}): \"{match_result[:30]}...\"")
                else:
                    print(f"âŒ ç¼©è¿›æœªåŒ¹é…: \"{para_text[:30]}...\"")
                    # é€šç”¨è°ƒè¯•æ¡ä»¶ï¼šåŸºäºæ–‡æœ¬ç‰¹å¾å’Œå¤æ‚åº¦åˆ¤æ–­æ˜¯å¦éœ€è¦è¯¦ç»†è°ƒè¯•
                    should_debug = self._should_enable_detailed_debug(para_text)
                    if should_debug:
                        print(f"  â†’ è¯¦ç»†è°ƒè¯•åŒ¹é…è¿‡ç¨‹:")
                        print(f"     åŸæ–‡æœ¬: {repr(para_text[:80])}")
                        print(f"     æ–‡æœ¬é•¿åº¦: {len(para_text)}")
                        
                        # æ˜¾ç¤ºå„ç§æ¸…ç†æ­¥éª¤
                        cleaned_text = self._clean_dot_below_markers(para_text)
                        normalized_text = self._normalize_quotes(para_text)
                        fully_cleaned = self._normalize_quotes(cleaned_text)
                        
                        print(f"     DOT_BELOWæ¸…ç†å: {repr(cleaned_text[:80])}")
                        print(f"     å¼•å·æ ‡å‡†åŒ–å: {repr(normalized_text[:80])}")
                        print(f"     å®Œå…¨æ¸…ç†å: {repr(fully_cleaned[:80])}")
                        
                        # æ£€æŸ¥å„ç§åŒ¹é…å¯èƒ½æ€§
                        content_lines = content.split('\n')
                        found_similar = []
                        
                        for i, line in enumerate(content_lines):
                            line_stripped = line.strip()
                            if not line_stripped:
                                continue
                                
                            # æ£€æŸ¥å„ç§åŒ¹é…ï¼ˆç§»é™¤ç¡¬ç¼–ç çš„å†…å®¹åˆ¤æ–­ï¼‰
                            matches = []
                            if cleaned_text in line_stripped:
                                matches.append("DOT_BELOWæ¸…ç†")
                            if fully_cleaned in line_stripped:
                                matches.append("å®Œå…¨æ¸…ç†")
                            if para_text[:20] in line_stripped:
                                matches.append("å‰20å­—ç¬¦")
                            if line_stripped[:20] in para_text:
                                matches.append("è¡Œå‰20å­—ç¬¦")
                                
                            # é€šç”¨ç›¸ä¼¼åº¦æ£€æŸ¥ï¼Œç§»é™¤å…·ä½“å†…å®¹åˆ¤æ–­
                            if matches or self._has_high_text_similarity(para_text, line_stripped):
                                found_similar.append(f"     ç¬¬{i+1}è¡Œ: {repr(line_stripped[:80])} [{', '.join(matches) if matches else 'é«˜ç›¸ä¼¼åº¦'}]")
                        
                        if found_similar:
                            print(f"     æ‰¾åˆ°ç›¸ä¼¼å†…å®¹:")
                            for similar in found_similar[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                                print(similar)
                        else:
                            print(f"     âŒ æœªæ‰¾åˆ°ä»»ä½•ç›¸ä¼¼å†…å®¹")
                            
                        # å°è¯•æ¨¡ç³ŠåŒ¹é…
                        for j, line in enumerate(content_lines):
                            line_stripped = line.strip()
                            if len(line_stripped) > 10 and abs(len(line_stripped) - len(para_text)) < 10:
                                # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„å­—ç¬¦é‡å ï¼‰
                                overlap = len(set(para_text) & set(line_stripped))
                                if overlap > len(para_text) * 0.7:
                                    print(f"     ğŸ” é«˜ç›¸ä¼¼åº¦è¡Œ: {repr(line_stripped[:60])} (é‡å å­—ç¬¦: {overlap}/{len(para_text)})")
                                    cleaned_line = self._clean_dot_below_markers(line)
                                    print(f"     ç¬¬{j+1}è¡Œæ¸…ç†å: {repr(cleaned_line[:80])}")
        
        # ç¬¬äºŒæ­¥ï¼šå¤„ç†æ–‡æœ¬ç‰¹æ®Šæ ¼å¼
        # æŒ‰æ–‡æœ¬é•¿åº¦æ’åºï¼Œä»é•¿åˆ°çŸ­ï¼Œé¿å…çŸ­æ–‡æœ¬æ›¿æ¢å½±å“é•¿æ–‡æœ¬
        sorted_formats = sorted(self.special_formatted_text, 
                              key=lambda x: len(x['text']), reverse=True)
        
        format_enhanced_count = 0
        
        for format_item in sorted_formats:
            text = format_item['text'].strip()
            formats = format_item['formats']
            
            # è·³è¿‡ç©ºæ–‡æœ¬æˆ–è¿‡çŸ­æ–‡æœ¬
            if len(text) < 2:
                continue
            
            # ç”Ÿæˆæ ¼å¼æ ‡æ³¨
            format_annotation = self._generate_format_annotation(formats)
            
            if format_annotation and text in content:
                # åˆ›å»ºå¢å¼ºçš„æ–‡æœ¬æ ‡æ³¨
                enhanced_text = f"[{text}]{{{format_annotation}}}"
                
                # æ‰§è¡Œæ›¿æ¢ï¼ˆåªæ›¿æ¢ç¬¬ä¸€ä¸ªåŒ¹é…ï¼Œé¿å…é‡å¤æ›¿æ¢ï¼‰
                content = content.replace(text, enhanced_text, 1)
                format_enhanced_count += 1
                
                print(f"æ ¼å¼å¢å¼º: \"{text[:30]}{'...' if len(text) > 30 else ''}\" -> {format_annotation}")
        
        print(f"âœ… æ ¼å¼å¢å¼ºå®Œæˆ:")
        print(f" å±…å³å¯¹é½æ ‡è®°: {right_aligned_enhanced_count} ä¸ªæ®µè½")
        print(f" å±…ä¸­å¯¹é½æ ‡è®°: {centered_enhanced_count} ä¸ªæ®µè½")
        print(f" é¦–è¡Œç¼©è¿›æ ‡è®°: {indent_enhanced_count} ä¸ªæ®µè½")
        print(f" ç‰¹æ®Šæ ¼å¼æ ‡è®°: {format_enhanced_count} ä¸ªæ–‡æœ¬")
        return content
    
    def _generate_format_annotation(self, formats):
        """æ ¹æ®æ ¼å¼åˆ—è¡¨ç”Ÿæˆæ ‡æ³¨"""
        annotations = []
        
        # å¤„ç†ä¸‹åˆ’çº¿ç±»å‹
        for fmt in formats:
            if "æ³¢æµªçº¿æ ¼å¼" in fmt:
                annotations.append(".wavy-underline")
            elif "ç‚¹çŠ¶çº¿æ ¼å¼" in fmt:
                annotations.append(".dotted-underline")
            elif "ä¸‹åˆ’çº¿: å•ä¸‹åˆ’çº¿" in fmt:
                annotations.append(".single-underline")
            elif "ä¸‹åˆ’çº¿: åŒä¸‹åˆ’çº¿" in fmt:
                annotations.append(".double-underline")
            elif "ä¸‹åˆ’çº¿: ç²—ä¸‹åˆ’çº¿" in fmt:
                annotations.append(".thick-underline")
            elif "ä¸‹åˆ’çº¿: è™šçº¿ä¸‹åˆ’çº¿" in fmt:
                annotations.append(".dashed-underline")
            elif "åˆ é™¤çº¿" in fmt:
                annotations.append(".strikethrough")
            elif "ä¸Šæ ‡" in fmt:
                annotations.append(".superscript")
            elif "ä¸‹æ ‡" in fmt:
                annotations.append(".subscript")
            elif "ç€é‡å·" in fmt:
                annotations.append(".emphasis-mark")
            elif "ç²—ä½“" in fmt:
                annotations.append(".bold")
            elif "æ–œä½“" in fmt:
                annotations.append(".italic")
        
        # å¤„ç†å­—ä½“é¢œè‰²ï¼ˆæå–é¢œè‰²å€¼ï¼‰
        for fmt in formats:
            if "å­—ä½“é¢œè‰²:" in fmt and "000000" not in fmt:  # è·³è¿‡é»‘è‰²
                color = fmt.split("å­—ä½“é¢œè‰²:")[-1].strip()
                annotations.append(f".color-{color}")
        
        # å¤„ç†å­—å·
        for fmt in formats:
            if "å­—å·:" in fmt and "ç£…" in fmt:
                size = fmt.split("å­—å·:")[-1].replace("ç£…", "").strip()
                try:
                    size_num = float(size)
                    if size_num != 12.0:  # è·³è¿‡é»˜è®¤å­—å·
                        annotations.append(f".font-{size}pt")
                except:
                    pass
        
        return " ".join(annotations) if annotations else None
    
    def convert_word_to_markdown(self, file_path):
        """å°†Wordæ–‡æ¡£è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        return self.convert_word_to_text(file_path, 'markdown')
    
    def convert_word_to_plain_text(self, file_path):
        """å°†Wordæ–‡æ¡£è½¬æ¢ä¸ºçº¯æ–‡æœ¬æ ¼å¼"""
        return self.convert_word_to_text(file_path, 'plain')
    
    def convert_word_to_html(self, file_path):
        """å°†Wordæ–‡æ¡£è½¬æ¢ä¸ºHTMLæ ¼å¼"""
        return self.convert_word_to_text(file_path, 'html')
    
    def call_llm_api(self, content, prompt_template_path="prompt.md"):
        """è°ƒç”¨å¤§æ¨¡å‹APIè§£ææ–‡æ¡£ç»“æ„"""
        print("å¼€å§‹è°ƒç”¨å¤§æ¨¡å‹API...")
        
        # è¯»å–promptæ¨¡æ¿
        try:
            with open(prompt_template_path, 'r', encoding='utf-8') as f:
                prompt_template = f.read()
            # ä½¿ç”¨å®‰å…¨çš„å­—ç¬¦ä¸²æ›¿æ¢
            prompt = prompt_template.replace("{content}", content)
            print(f"æˆåŠŸåŠ è½½promptæ¨¡æ¿: {prompt_template_path}")
        except FileNotFoundError:
            print(f"âŒ æœªæ‰¾åˆ°promptæ¨¡æ¿æ–‡ä»¶: {prompt_template_path}")
            print("âŒ è¿™æ˜¯ä¸€ä¸ªä¸¥é‡é”™è¯¯ï¼å¿…é¡»ä½¿ç”¨æ­£ç¡®çš„promptæ¨¡æ¿ï¼")
            print("âŒ é»˜è®¤promptä¸ä¼˜åŒ–åçš„è¦æ±‚ä¸åŒ¹é…ï¼Œä¼šå¯¼è‡´é€‰é¡¹ç¼ºå¤±ç­‰é—®é¢˜")
            print("ğŸ’¡ è¯·ç¡®ä¿prompt_Chinese.mdæ–‡ä»¶å­˜åœ¨ä¸”å¯è¯»")
            return None
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        # å…ˆæµ‹è¯•APIè¿æ¥
        test_data = {
            "model": "doubao-seed-1-6-250615",
            "messages": [{"role": "user", "content": "Hello"}],
            "max_tokens": 32000,
            "temperature": 0.1,
            "stream": True,
            "thinking": {
                "type": "enabled",
                "budget_tokens": 2000
            }
        }
        
        print("ğŸ” æµ‹è¯•APIè¿æ¥...")
        try:
            test_response = requests.post(
                f"{self.base_url}/v3/chat/completions", 
                headers=headers, 
                json=test_data, 
                timeout=30
            )
            test_response.raise_for_status()
            print("âœ… APIè¿æ¥æµ‹è¯•æˆåŠŸ")
        except requests.exceptions.HTTPError as e:
            print(f"âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            print(f"å“åº”çŠ¶æ€ç : {test_response.status_code}")
            print(f"å“åº”å†…å®¹: {test_response.text}")
            return None
        except Exception as e:
            print(f"âŒ APIè¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
            return None
        
        # æ­£å¼è¯·æ±‚ - ä½¿ç”¨æµå¼è¾“å‡º
        data = {
            "model": "doubao-seed-1-6-250615",
            "messages": [{"role": "user", "content": prompt}],
            "stream": True,
            "thinking": {
                "type": "enabled",
                "budget_tokens": 1500
            },
            "response_format": {
                "type": "json_object"
            },
            "temperature": 0.1,
            "max_completion_tokens": 32000
        }
        
        print("ğŸš€ è°ƒç”¨å¤§æ¨¡å‹APIè§£ææ–‡æ¡£ç»“æ„...")
        print(f"è¯·æ±‚URL: {self.base_url}/v3/chat/completions")
        print(f"æ¨¡å‹: {data['model']}")
        print(f"æ¶ˆæ¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        try:
            response = requests.post(
                f"{self.base_url}/v3/chat/completions", 
                headers=headers, 
                json=data, 
                timeout=300,  # å‡å°‘è¶…æ—¶æ—¶é—´åˆ°5åˆ†é’Ÿ
                stream=True
            )
            response.raise_for_status()
            
            # å¤„ç†æµå¼å“åº”
            llm_content = ""
            print("ğŸ“¡ å¼€å§‹æ¥æ”¶æµå¼å“åº”...")
            
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data_str = line[6:]  # å»æ‰ 'data: ' å‰ç¼€
                        
                        if data_str == '[DONE]':
                            print("\nâœ… æµå¼å“åº”æ¥æ”¶å®Œæˆ")
                            break
                        
                        try:
                            data_json = json.loads(data_str)
                            if 'choices' in data_json and len(data_json['choices']) > 0:
                                choice = data_json['choices'][0]
                                
                                # å¤„ç†thinkingçŠ¶æ€
                                if 'thinking' in choice:
                                    thinking = choice['thinking']
                                    if thinking.get('type') == 'thinking':
                                        print(f"ğŸ¤” æ€è€ƒä¸­... ({thinking.get('tokens_used', 0)} tokens)")
                                    elif thinking.get('type') == 'finished':
                                        print(f"âœ… æ€è€ƒå®Œæˆï¼Œå…±ä½¿ç”¨ {thinking.get('tokens_used', 0)} tokens")
                                
                                # å¤„ç†deltaå†…å®¹
                                if 'delta' in choice and 'content' in choice['delta']:
                                    content = choice['delta']['content']
                                    llm_content += content
                                    print(content, end='', flush=True)
                                    
                        except json.JSONDecodeError:
                            continue
            
            print(f"\nâœ… APIè°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(llm_content)} å­—ç¬¦")
            return llm_content
            
        except requests.exceptions.HTTPError as e:
            print(f"HTTPé”™è¯¯: {e}")
            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”å†…å®¹: {response.text}")
            return None
        except Exception as e:
            print(f"âŒ APIè°ƒç”¨å¼‚å¸¸: {e}")
            return None
    
    def process_word_document(self, file_path, output_format='markdown', prompt_template_path="prompt.md", 
                            enable_dot_below_detection=True, enable_coze_workflow=True):
        """å®Œæ•´çš„Wordæ–‡æ¡£å¤„ç†æµç¨‹"""
        print("=" * 60)
        print("Pandoc Wordæ–‡æ¡£å¤„ç†å·¥å…· - å¢å¼ºç‰ˆ (æ”¯æŒåŠ ç‚¹å­—)")
        print("=" * 60)
        print(f"æ–‡æ¡£æ–‡ä»¶: {file_path}")
        print(f"è¾“å‡ºæ ¼å¼: {output_format}")
        print(f"Promptæ¨¡æ¿: {prompt_template_path}")
        # print(f"æ ¼å¼åˆ†æ: å·²ç¦ç”¨")  # å·²ç§»é™¤æ ¼å¼åˆ†æåŠŸèƒ½
        print(f"æ ¼å¼æ ‡è®°: å¯ç”¨å¢å¼ºæ ‡æ³¨ï¼ˆç”Ÿæˆpandocæ ¼å¼ï¼‰")
        print(f"åŠ ç‚¹å­—æ ‡è®°ä¿ç•™: {'å¯ç”¨' if enable_dot_below_detection else 'ç¦ç”¨'}")
        print(f"Cozeå·¥ä½œæµ: {'å¯ç”¨' if enable_coze_workflow else 'ç¦ç”¨'}")
        print("=" * 60)
        
        # ç¬¬ä¸€æ­¥ - åŠ ç‚¹å­—é¢„å¤„ç†ï¼ˆå¦‚æœå¯ç”¨ä¸”ä¸ºdocxæ–‡ä»¶ï¼‰
        processed_file_path = file_path
        if enable_dot_below_detection and file_path.lower().endswith('.docx'):
            processed_file_path = self._preprocess_dot_below_chars(file_path)
            if not processed_file_path:
                processed_file_path = file_path  # å›é€€åˆ°åŸæ–‡ä»¶
        
        # ç¬¬äºŒæ­¥ - æ ¼å¼åˆ†æï¼ˆå¦‚æœå¯ç”¨ä¸”ä¸ºdocxæ–‡ä»¶ï¼‰
        format_analysis = None
        if processed_file_path.lower().endswith('.docx'):
            format_analysis = self.extract_format_analysis(processed_file_path)
        
        # ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨pandocè½¬æ¢æ–‡æ¡£
        content = self.convert_word_to_text(processed_file_path, output_format)
        if not content:
            print("âŒ æ–‡æ¡£è½¬æ¢å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
            return None
        
        # ç¬¬å››æ­¥ï¼šè½¬æ¢è¿ç»­çŸ­æ¨ªçº¿ä¸ºä¸­æ–‡ç ´æŠ˜å·
        content = self._convert_dashes_to_chinese(content)
        
        # ç¬¬å…­æ­¥ï¼šè°ƒç”¨å¤§æ¨¡å‹APIè§£æå†…å®¹
        llm_response = self.call_llm_api(content, prompt_template_path)
        if not llm_response:
            print("âŒ APIè°ƒç”¨å¤±è´¥")
            return None
        
        # ç¬¬ä¸ƒæ­¥ï¼šå¤„ç†APIå“åº”å¹¶é›†æˆæ ¼å¼ä¿¡æ¯
        api_result = self._process_api_response(llm_response, file_path)
        
        # ç¬¬å…«æ­¥ï¼šå¦‚æœæœ‰æ ¼å¼åˆ†æç»“æœï¼Œå°†å…¶æ•´åˆåˆ°æœ€ç»ˆç»“æœä¸­
        if format_analysis and api_result:
            print("ğŸ”— æ ¼å¼ä¿¡æ¯æ•´åˆå®Œæˆ")
        
        # ç¬¬ä¹æ­¥ï¼šè°ƒç”¨Cozeå·¥ä½œæµï¼ˆå¦‚æœå¯ç”¨ï¼‰
        coze_ids = None
        if enable_coze_workflow:
            print("\n" + "=" * 60)
            print("ğŸ”— Cozeå·¥ä½œæµå¤„ç†é˜¶æ®µ")
            print("=" * 60)
            
            if api_result:
                # æ­£å¸¸æƒ…å†µï¼šä½¿ç”¨APIè§£æç»“æœè°ƒç”¨Coze
                coze_ids = self.call_coze_workflow(api_result)
                
                if coze_ids:
                    # åˆ›å»ºcoze_resæ–‡ä»¶å¤¹
                    coze_res_dir = Path("coze_res")
                    coze_res_dir.mkdir(exist_ok=True)
                    
                    # å°†Cozeè¿”å›çš„IDåˆ—è¡¨ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    coze_output_file = coze_res_dir / f"coze_ids_{Path(file_path).stem}_{timestamp}.txt"
                    
                    with open(coze_output_file, 'w', encoding='utf-8') as f:
                        f.write(",".join(coze_ids))
                    
                    print(f"ğŸ“ Coze IDåˆ—è¡¨å·²ä¿å­˜åˆ°: {coze_output_file}")
                    
                    # å¯é€‰ï¼šå°†IDåˆ—è¡¨æ·»åŠ åˆ°APIç»“æœä¸­
                    if isinstance(api_result, list):
                        # å¦‚æœAPIç»“æœæ˜¯é¢˜ç›®åˆ—è¡¨ï¼Œå¯ä»¥ä¸ºæ¯é“é¢˜æ·»åŠ ä¸€ä¸ªID
                        for i, question in enumerate(api_result[:len(coze_ids)]):
                            if isinstance(question, dict):
                                question['coze_id'] = coze_ids[i] if i < len(coze_ids) else None
                    print("âœ… Coze IDå·²æ•´åˆåˆ°é¢˜ç›®ç»“æœä¸­")
                else:
                    print("âš ï¸ Cozeå·¥ä½œæµæœªè¿”å›æœ‰æ•ˆæ•°æ®")
            else:
                # APIè§£æå¤±è´¥çš„æƒ…å†µï¼šæä¾›æ‰‹åŠ¨è°ƒç”¨æŒ‡å¯¼
                print("âš ï¸ ç”±äºJSONè§£æå¤±è´¥ï¼Œæ— æ³•è‡ªåŠ¨è°ƒç”¨Cozeå·¥ä½œæµ")
                print("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
                print("  1. æ£€æŸ¥å¹¶ä¿®å¤ç”Ÿæˆçš„JSONæ–‡ä»¶æ ¼å¼é—®é¢˜")
                print("  2. æˆ–è€…ä½¿ç”¨æ‰‹åŠ¨è„šæœ¬è°ƒç”¨Cozeå·¥ä½œæµ:")
                print("     python3 manual_coze_call.py")
        
        return {
            'questions': api_result,
            'coze_ids': coze_ids
        } if enable_coze_workflow else api_result
    
    def _process_api_response(self, llm_content, original_file_path):
        """å¤„ç†APIå“åº”å¹¶ä¿å­˜ç»“æœï¼Œå¢å¼ºå¥å£®æ€§å’Œé”™è¯¯å¤„ç†"""
        def extract_json_from_codeblock(content):
            """ä»Markdownä»£ç å—ä¸­æå–JSONå†…å®¹ï¼Œå¤„ç†å¤šç§æ ¼å¼æƒ…å†µ"""
            # åŒ¹é…```jsonå¼€å¤´çš„ä»£ç å—ï¼ˆå…è®¸å‰åæœ‰ç©ºç™½ï¼‰
            json_block_pattern = re.compile(r'^\s*```\s*json\s*\n(.*?)\n\s*```\s*$', re.DOTALL | re.MULTILINE)
            match = json_block_pattern.search(content)
            if match:
                return match.group(1).strip()
            
            # åŒ¹é…æ™®é€š```ä»£ç å—
            general_block_pattern = re.compile(r'^\s*```\s*\n(.*?)\n\s*```\s*$', re.DOTALL | re.MULTILINE)
            match = general_block_pattern.search(content)
            if match:
                return match.group(1).strip()
            
            # æ— ä»£ç å—æ—¶è¿”å›åŸå§‹å†…å®¹ï¼ˆå¯èƒ½å·²æ˜¯çº¯JSONï¼‰
            return content.strip()

        def clean_json_string(content):
            """æ¸…ç†JSONå­—ç¬¦ä¸²ï¼Œå¤„ç†å¸¸è§æ ¼å¼é—®é¢˜"""
            # ç§»é™¤JSONä¸­çš„æ³¨é‡Šï¼ˆ/* ... */ æˆ– // ...ï¼‰
            content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
            content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)
            
            # ç§»é™¤å°¾é€—å·ï¼ˆå¦‚ [1,2,] æˆ– {"a":1,}ï¼‰
            content = re.sub(r',\s*([}\]])', r'\1', content)
            
            return content.strip()

        # 1. æå–å¹¶æ¸…ç†å†…å®¹
        try:
            # æå–JSONå†…å®¹ï¼ˆå¤„ç†ä»£ç å—ï¼‰
            extracted_content = extract_json_from_codeblock(llm_content)
            
            # æ¸…ç†JSONæ ¼å¼é—®é¢˜
            cleaned_content = clean_json_string(extracted_content)
        except Exception as e:
            print(f"âš ï¸ å†…å®¹æå–/æ¸…ç†å¤±è´¥: {e}")
            cleaned_content = llm_content.strip()

        # 2. ä¿å­˜åŸå§‹å“åº”ï¼ˆæ— è®ºåç»­å¤„ç†æ˜¯å¦æˆåŠŸï¼Œä¾¿äºè°ƒè¯•ï¼‰
        try:
            # åˆ›å»ºä¸“é—¨çš„åŸå§‹å“åº”ç›®å½•
            raw_dir = Path("raw_api_responses")
            raw_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            raw_filename = raw_dir / f"raw_response_{Path(original_file_path).stem}_{timestamp}.txt"
            with open(raw_filename, 'w', encoding='utf-8') as f:
                f.write(f"=== åŸå§‹LLMå“åº” ===\n{llm_content}\n\n")
                f.write(f"=== æå–åå†…å®¹ ===\n{extracted_content}\n\n")
                f.write(f"=== æ¸…ç†åå†…å®¹ ===\n{cleaned_content}")
            print(f"ğŸ“„ åŸå§‹å“åº”å·²ä¿å­˜åˆ°: {raw_filename}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜åŸå§‹å“åº”å¤±è´¥: {e}")

        # 3. è§£æJSONå¹¶ä¿å­˜ç»“æœ
        try:
            # caution: å¦‚æœç›´æ¥ä½¿ç”¨json.loads(cleaned_content)ï¼Œä¼šæŠ¥é”™ï¼Œå› ä¸ºcleaned_contentæ˜¯å­—ç¬¦ä¸²ï¼Œä¸æ˜¯JSONå¯¹è±¡   
            # questions = json.loads(cleaned_content)
            
            # å¯¹questionsè¿›è¡ŒJSONå†…å®¹åå¤„ç†ï¼šä¸­æ–‡å¼•å·ã€çœç•¥å·ã€ä¸Šè§’æ ‡ç­‰æ ¼å¼è½¬æ¢
            questions = post_process_json_content(cleaned_content)

            # éªŒè¯è§£æç»“æœæ ¼å¼
            if not isinstance(questions, list):
                raise ValueError("è§£æç»“æœä¸æ˜¯JSONæ•°ç»„")
            
            # ä¿å­˜å¤„ç†åçš„ç»“æœ
            json_res_dir = Path("json_res")
            json_res_dir.mkdir(exist_ok=True)
            
            output_file = json_res_dir / f"questions_{Path(original_file_path).stem}_{timestamp}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(questions, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ‰ å®Œæˆï¼å…±{len(questions)}é“é¢˜ç›®ï¼Œä¿å­˜åˆ°: {output_file}")
            return questions
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            print(f"é”™è¯¯ä½ç½®: ç¬¬{e.lineno}è¡Œç¬¬{e.colno}åˆ—")
            print(f"è¯·æŸ¥çœ‹åŸå§‹å“åº”æ–‡ä»¶: {raw_filename}")
            return None
        except ValueError as e:
            print(f"âŒ è§£æç»“æœæ ¼å¼é”™è¯¯: {e}")
            return None
        except Exception as e:
            print(f"âŒ å¤„ç†å“åº”æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            return None
    
    def call_coze_workflow(self, processed_data):
        """è°ƒç”¨Cozeå·¥ä½œæµ"""
        print("ğŸ”— å‡†å¤‡è°ƒç”¨Cozeå·¥ä½œæµ...")
        
        try:
            headers = {
                'Authorization': f'Bearer pat_Z0r3WQNZ435IUDhJCc0bVHDd9mVcIh0Z6tOvYd3HPT3Q6WNfw5KaX7veOhNkqC3N',
                'Content-Type': 'application/json'
            }

            data = {
                "workflow_id": "7540878860784680995",
                "parameters": {
                    "input": json.dumps(processed_data, ensure_ascii=False)
                }
            }
            
            print("ğŸš€ è°ƒç”¨Cozeå·¥ä½œæµå¼€å§‹...")
            print(f"ğŸ“Š å‘é€æ•°æ®é‡: {len(json.dumps(processed_data, ensure_ascii=False))} å­—ç¬¦")
            
            response = requests.post('https://api.coze.cn/v1/workflow/run', headers=headers, data=json.dumps(data))

            if response.status_code == 200:
                response_data = response.json().get("data")
                
                if response_data:
                    # è§£æ JSON å­—ç¬¦ä¸²
                    parsed_data = json.loads(response_data)
                    
                    # æå– data å­—æ®µå¹¶æŒ‰ \n åˆ†å‰²æˆæ•°ç»„
                    id_list = parsed_data["data"].split("\n")
                    
                    print(f"âœ… Cozeå·¥ä½œæµè°ƒç”¨æˆåŠŸï¼Œè¿”å› {len(id_list)} ä¸ªID")
                    print(f"ğŸ“‹ IDåˆ—è¡¨é¢„è§ˆ: {', '.join(id_list[:5])}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
                    
                    return id_list
                else:
                    print("âŒ Cozeå·¥ä½œæµè¿”å›æ•°æ®ä¸ºç©º")
                    return None
            else:
                print(f"âŒ Cozeå·¥ä½œæµè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
                return None

        except Exception as e:
            print(f"âŒ è°ƒç”¨Cozeå·¥ä½œæµå¼‚å¸¸: {e}")
            return None
    
    def _preprocess_dot_below_chars(self, docx_path):
        """é¢„å¤„ç†docxæ–‡ä»¶ä¸­çš„åŠ ç‚¹å­—ï¼Œä½¿pandocèƒ½è¯†åˆ«"""
        print("ğŸ” é¢„å¤„ç†åŠ ç‚¹å­—...")
        
        try:
            # å¯¼å…¥é¢„å¤„ç†å™¨
            import zipfile
            import xml.etree.ElementTree as ET
            import tempfile
            import shutil
            import re
            
            # åˆ›å»ºä¸“é—¨çš„å­æ–‡ä»¶å¤¹æ¥å­˜å‚¨ä¸­é—´æ–‡ä»¶
            from pathlib import Path
            input_path = Path(docx_path)
            
            # å¦‚æœæ–‡ä»¶åœ¨Chineseæ–‡ä»¶å¤¹ä¸­ï¼Œåˆ›å»ºprocessedå­æ–‡ä»¶å¤¹
            if 'Chinese' in str(input_path):
                # è·å–Chineseæ–‡ä»¶å¤¹çš„è·¯å¾„
                chinese_folder = None
                for parent in input_path.parents:
                    if parent.name == 'Chinese':
                        chinese_folder = parent
                        break
                
                if chinese_folder:
                    processed_folder = chinese_folder / 'processed'
                    processed_folder.mkdir(exist_ok=True)
                    filename = input_path.name.replace('.docx', '_dot_processed.docx')
                    output_path = str(processed_folder / filename)
                    print(f"ä¸­é—´æ–‡ä»¶å°†ä¿å­˜åˆ°: processed/{filename}")
                else:
                    # å›é€€åˆ°åŸæ¥çš„æ–¹å¼
                    output_path = docx_path.replace('.docx', '_dot_processed.docx')
            else:
                # ä¸åœ¨Chineseæ–‡ä»¶å¤¹ä¸­ï¼Œä½¿ç”¨åŸæ¥çš„æ–¹å¼
                output_path = docx_path.replace('.docx', '_dot_processed.docx')
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•æ¥è§£å‹å’Œé‡æ–°æ‰“åŒ…docx
            with tempfile.TemporaryDirectory() as temp_dir:
                extract_dir = Path(temp_dir) / "docx_content"
                extract_dir.mkdir()
                
                # è§£å‹docxæ–‡ä»¶
                with zipfile.ZipFile(docx_path, 'r') as zip_file:
                    zip_file.extractall(extract_dir)
                
                # ä¿®æ”¹document.xml
                document_xml_path = extract_dir / "word" / "document.xml"
                if document_xml_path.exists():
                    with open(document_xml_path, 'r', encoding='utf-8') as f:
                        xml_content = f.read()
                    
                    # æŸ¥æ‰¾å¹¶æ›¿æ¢åŠ ç‚¹å­—æ ‡è®°
                    run_with_em_pattern = r'(<w:r>.*?<w:rPr>.*?)<w:em w:val="dot"\s*/>(.*?</w:rPr>.*?<w:t>)(.*?)(</w:t>.*?</w:r>)'
                    
                    def replace_run_with_em(match):
                        before_em = match.group(1)
                        after_em = match.group(2) 
                        text_content = match.group(3)
                        after_text = match.group(4)
                        
                        # æ·»åŠ ä¸‹åˆ’çº¿å’Œç‰¹æ®Šæ ‡è®°
                        underline_xml = '<w:u w:val="single"/>'
                        marked_text = f"[DOT_BELOW]{text_content}[/DOT_BELOW]"
                        
                        return f"{before_em}{underline_xml}{after_em}{marked_text}{after_text}"
                    
                    modified_content, replacement_count = re.subn(run_with_em_pattern, replace_run_with_em, xml_content, flags=re.DOTALL)
                    
                    if replacement_count > 0:
                        with open(document_xml_path, 'w', encoding='utf-8') as f:
                            f.write(modified_content)
                        print(f"  âœ… å¤„ç†äº† {replacement_count} ä¸ªåŠ ç‚¹å­—")
                
                # é‡æ–°æ‰“åŒ…docxæ–‡ä»¶
                with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                    for file_path in extract_dir.rglob('*'):
                        if file_path.is_file():
                            relative_path = file_path.relative_to(extract_dir)
                            zip_file.write(file_path, relative_path)
            
            return output_path
            
        except Exception as e:
            print(f"  âš ï¸ é¢„å¤„ç†å¤±è´¥: {e}")
            return None
    

    def _convert_dashes_to_chinese(self, content):
        """è½¬æ¢è¿ç»­çŸ­æ¨ªçº¿ä¸ºä¸­æ–‡ç ´æŠ˜å·"""
        print("ğŸ”€ è½¬æ¢è¿ç»­çŸ­æ¨ªçº¿ä¸ºä¸­æ–‡ç ´æŠ˜å·...")
        
        try:
            import re
            
            conversion_count = 0
            
            dash_pattern = r'-{3,}'  # åŒ¹é…3ä¸ªæˆ–æ›´å¤šè¿ç»­çš„çŸ­æ¨ªçº¿
            
            def replace_dashes(match):
                nonlocal conversion_count
                dashes = match.group(0)
                dash_count = len(dashes)
                conversion_count += 1
                # æ¯3ä¸ªçŸ­æ¨ªçº¿æ›¿æ¢ä¸ºä¸€ä¸ªem dash
                em_dash_count = dash_count // 3
                return 'â€”' * em_dash_count
            
            content = re.sub(dash_pattern, replace_dashes, content)
            
            if conversion_count > 0:
                print(f"  âœ… è½¬æ¢äº† {conversion_count} å¤„è¿ç»­çŸ­æ¨ªçº¿ä¸ºä¸­æ–‡ç ´æŠ˜å·ï¼ˆæ¯3ä¸ªçŸ­æ¨ªçº¿è½¬æ¢ä¸º1ä¸ªem dashï¼‰")
            else:
                print("  â„¹ï¸ æœªå‘ç°éœ€è¦è½¬æ¢çš„è¿ç»­çŸ­æ¨ªçº¿")
            
            return content
            
        except Exception as e:
            print(f"  âš ï¸ ç ´æŠ˜å·è½¬æ¢å¤±è´¥: {e}")
            return content

def post_process_json_content(data):
    """
    JSONå†…å®¹åå¤„ç†å‡½æ•°ï¼šå¯¹è§£æåçš„JSONæ•°æ®è¿›è¡Œæ ¼å¼è§„èŒƒåŒ–å¤„ç†
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. è‹±æ–‡å¼•å· â†’ ä¸­æ–‡å·¦å³å¼•å·ï¼šäº¤æ›¿å°†åŒå¼•å·"æ›¿æ¢ä¸º""ï¼Œå•å¼•å·'æ›¿æ¢ä¸º''
    2. è‹±æ–‡çœç•¥å· â†’ ä¸­æ–‡çœç•¥å·ï¼šå°†è¿ç»­å…­ä¸ªå¥ç‚¹......è½¬æ¢ä¸ºâ€¦â€¦
    3. ä¸Šè§’æ ‡æ ¼å¼è½¬æ¢ï¼šå°†^å†…å®¹^å½¢å¼è½¬æ¢ä¸º<sup>å†…å®¹</sup>HTMLæ ‡ç­¾
    4. æ‰‹åŠ¨HTMLè§£æç¡®ä¿åœ¨æ ‡ç­¾å†…éƒ¨ä¸è¿›è¡Œè½¬æ¢

    å‚æ•°:
        data: åŒ…å« HTML å†…å®¹çš„ JSON æ•°æ®ï¼ˆå­—å…¸ã€åˆ—è¡¨æˆ–å­—ç¬¦ä¸²ï¼‰

    è¿”å›:
        å¤„ç†åçš„ JSON æ•°æ®ï¼Œæ‰€æœ‰æ–‡æœ¬å†…å®¹å·²å®Œæˆæ ¼å¼è§„èŒƒåŒ–
    """

    def replace_quotes_in_html(html_content):
        """æ‰‹åŠ¨è§£æ HTML å†…å®¹å¹¶è¿›è¡Œå¤šç§æ ¼å¼è½¬æ¢ï¼šå¼•å·ã€çœç•¥å·ã€ä¸Šè§’æ ‡ç­‰"""
        if not html_content:
            return html_content

        try:
            result = []
            i = 0
            n = len(html_content)
            single_quote_count = 0  # ç”¨äºè·Ÿè¸ªå¼•å·å‡ºç°æ¬¡æ•°
            double_quote_count = 0  # ç”¨äºè·Ÿè¸ªå¼•å·å‡ºç°æ¬¡æ•°

            while i < n:
                if html_content[i] == '<':
                    # å¤„ç†æ ‡ç­¾éƒ¨åˆ†ï¼ˆåŸæ ·ä¿ç•™ï¼‰
                    tag_end = html_content.find('>', i)
                    if tag_end == -1:
                        tag_end = n
                    result.append(html_content[i:tag_end + 1])
                    i = tag_end + 1
                else:
                    # å¤„ç†æ–‡æœ¬å†…å®¹
                    text_end = html_content.find('<', i)
                    if text_end == -1:
                        text_end = n
                    text = html_content[i:text_end]

                    # å¤„ç†æ–‡æœ¬ä¸­çš„å¼•å·ï¼ˆäº¤æ›¿æ›¿æ¢ï¼‰ã€çœç•¥å·å’Œä¸Šè§’æ ‡
                    new_text = []
                    j = 0
                    text_len = len(text)
                    
                    while j < text_len:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸Šè§’æ ‡æ ¼å¼ ^å†…å®¹^
                        if text[j] == '^':
                            # å¯»æ‰¾å¯¹åº”çš„ç»“æŸ^ï¼Œç¡®ä¿ä¸è¶Šç•Œ
                            if j + 2 < text_len and text[j + 2] == '^':
                                content = text[j+1]
                                new_text.append(f'<sup>{content}</sup>')
                                j = j + 3  # è·³è¿‡ç»“æŸçš„^
                            else:
                                # æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ç»“æŸ^ï¼Œä¿ç•™åŸæ ·
                                new_text.append(text[j])
                                j += 1
                        # æ£€æŸ¥æ˜¯å¦æ˜¯å…­ä¸ªè¿ç»­çš„å¥ç‚¹
                        elif text[j] == '.' and j + 5 < text_len and all(text[j + k] == '.' for k in range(6)):
                            new_text.append('â€¦â€¦')
                            j += 6  # è·³è¿‡è¿™å…­ä¸ªå¥ç‚¹
                        else:
                            # å¤„ç†å¼•å·
                            char = text[j]
                            if char == '"':
                                double_quote_count += 1
                                converted_quote = 'â€œ' if double_quote_count % 2 == 1 else 'â€'
                                new_text.append(converted_quote)
                            elif char == "'":
                                single_quote_count += 1
                                converted_quote = "â€˜" if single_quote_count % 2 == 1 else "â€™"
                                new_text.append(converted_quote)
                            else:
                                new_text.append(char)
                            j += 1

                    result.append(''.join(new_text))
                    i = text_end

            return ''.join(result)
        except Exception as e:
            print(f"âš ï¸ HTMLå†…å®¹å¤„ç†å¤±è´¥: {e}")
            print(f"é—®é¢˜å†…å®¹: {repr(html_content[:100])}")
            return html_content  # å‡ºé”™æ—¶è¿”å›åŸå§‹å†…å®¹

    # å¦‚æœæ˜¯å­—ç¬¦ä¸²å½¢å¼çš„ JSONï¼Œå…ˆè§£æä¸ºå­—å…¸
    if isinstance(data, str):
        try:
            data = json.loads(data)
        except json.JSONDecodeError:
            return data

    # é€’å½’å¤„ç† JSON ä¸­çš„æ¯ä¸ªå­—æ®µ
    def process_item(item):
        try:
            if isinstance(item, dict):
                for key, value in item.items():
                    try:
                        if isinstance(value, str):
                            item[key] = replace_quotes_in_html(value)
                        elif isinstance(value, (dict, list)):
                            process_item(value)
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†å­—æ®µ {key} å¤±è´¥: {e}")
                        # å­—æ®µå¤„ç†å¤±è´¥æ—¶ä¿ç•™åŸå€¼ï¼Œä¸å½±å“å…¶ä»–å­—æ®µ
            elif isinstance(item, list):
                for i in range(len(item)):
                    try:
                        if isinstance(item[i], str):
                            item[i] = replace_quotes_in_html(item[i])
                        elif isinstance(item[i], (dict, list)):
                            process_item(item[i])
                    except Exception as e:
                        print(f"âš ï¸ å¤„ç†æ•°ç»„å…ƒç´  {i} å¤±è´¥: {e}")
                        # æ•°ç»„å…ƒç´ å¤„ç†å¤±è´¥æ—¶ä¿ç•™åŸå€¼ï¼Œä¸å½±å“å…¶ä»–å…ƒç´ 
        except Exception as e:
            print(f"âš ï¸ å¤„ç†æ•°æ®é¡¹å¤±è´¥: {e}")
            # æ•´ä½“å¤„ç†å¤±è´¥æ—¶ä»€ä¹ˆéƒ½ä¸åšï¼Œä¿æŒåŸå§‹æ•°æ®

    process_item(data)

    return data
    


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    # é…ç½®å‚æ•°
    if len(sys.argv) > 1:
        word_file_path = sys.argv[1]
    else:
        word_file_path = "Chinese/ç²¾å“è§£æï¼š2025å¹´å››å·çœå®œå®¾å¸‚ä¸­è€ƒè¯­æ–‡çœŸé¢˜ï¼ˆè§£æç‰ˆï¼‰.docx"  # é»˜è®¤æ–‡ä»¶è·¯å¾„
     
    output_format = "markdown"  # å¯é€‰: markdown, plain, html
    prompt_template_path = "prompt_Chinese.md"
    
    # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
    processor = PandocWordProcessor()
    
    # æ£€æŸ¥pandocå¯ç”¨æ€§
    if not processor.pandoc_available:
        print("âŒ Pandocä¸å¯ç”¨ï¼Œè¯·å…ˆå®‰è£…pandoc")
        print("å®‰è£…æ–¹æ³•:")
        print("  macOS: brew install pandoc")
        print("  Ubuntu/Debian: sudo apt-get install pandoc")
        print("  Windows: ä¸‹è½½å®‰è£…åŒ… https://pandoc.org/installing.html")
        return
    
    # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
    if not processor.is_supported_format(word_file_path):
        print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {Path(word_file_path).suffix}")
        print(f"è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ä¹‹ä¸€: {', '.join(processor.get_supported_formats())}")
        return
    
    # å¤„ç†æ–‡æ¡£
    result = processor.process_word_document(
        word_file_path, 
        output_format, 
        prompt_template_path
    )
    
    if result:
        print("âœ… æ–‡æ¡£å¤„ç†å®Œæˆï¼")
        
        # æ˜¾ç¤ºç‰¹æ®Šæ ¼å¼æ‘˜è¦
        format_summary = processor.get_special_format_summary()
        print("\n" + "="*50)
        print(format_summary)
        print("="*50)
    else:
        print("âŒ æ–‡æ¡£å¤„ç†å¤±è´¥")

if __name__ == "__main__":
    main() 